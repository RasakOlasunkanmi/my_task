If you master these 6 areas, youâ€™ll be able to handle almost any real-world dataset:





Data Cleaning â†’ trust your data.



EDA â†’ discover patterns.



Wrangling â†’ organize for analysis.



Transformation â†’ enrich features.



Visualization â†’ tell the story.



Stats â†’ validate findings.





###### Inspection \& structure

###### Data Exploration

###### Data Cleaning

###### Data Wrangling

###### Exploratory Data Analysis

###### Data Transformation

###### Advance Operations







A.	inspecting and checking the structure of the dataset



1\. `df.head()` â€“ first 5 rows

2\. `df.tail()` â€“ last 5 rows

3\. `df.sample(n)` â€“ random n rows

4\. `df.shape` â€“ dimensions (rows, columns)

5\. `df.columns` â€“ column names

6\. `df.index` â€“ index info

7\. `df.dtypes` â€“ column data types

8\. `df.info()` â€“ summary of dataset

9\. `df.describe()` â€“ statistical summary (numeric)

10\. `df.describe(include="object")` â€“ summary of categorical data

11\. `df.nunique()` â€“ number of unique values per column

12\. `df.isna().sum()` â€“ missing values count per column

13\. `df.memory\\\_usage(deep=True)` â€“ memory usage

14\. `df.value\\\_counts()` â€“ frequency counts (for a column)

15\. `df.corr()` â€“ correlation between numerical features

16\. `df.cov()` â€“ covariance matrix

17\. `df.duplicated().sum()` â€“ number of duplicate rows

18\. `df.apply(type)` â€“ check Python object types in each column

19\. `df.mode()` â€“ most frequent values

20\. `df.var()` / `df.std()` â€“ variance and standard deviation





---

ðŸ”Ž Data Exploration Techniques (20+)

1\. Dataset Overview



df.head() â†’ preview first 5 rows



df.tail() â†’ preview last 5 rows



df.sample(10) â†’ random sample of rows



df.shape â†’ number of rows \& columns



df.columns â†’ list column names



df.info() â†’ concise summary (types, nulls, memory)



df.dtypes â†’ data types of each column



df.index â†’ index details



2\. Descriptive Statistics



df.describe() â†’ summary stats (numeric)-



df.describe(include='object') â†’ stats for categorical data-



df.mean() / df.median() â†’ average / median of numeric columns



df.std() / df.var() â†’ spread of data



df.min() / df.max() â†’ min and max values



df.quantile(\[0.25, 0.5, 0.75]) â†’ percentiles



3\. Distribution \& Frequency



df\['col'].value\_counts() â†’ frequency of values



df\['col'].unique() â†’ unique values



df\['col'].nunique() â†’ number of unique values



df.mode() â†’ most frequent value(s)



4\. Relationships Between Features



df.corr() â†’ correlation matrix (numeric)



sns.heatmap(df.corr(), annot=True) â†’ visualize correlation



pd.crosstab(df\['col1'], df\['col2']) â†’ cross-tabulation



df.groupby('col').mean() â†’ aggregate by groups



df.pivot\_table(values='Amount', index='Customer', columns='Branch') â†’ pivot summaries



5\. Data Quality Checks



df.isna().sum() â†’ missing values per column



df.duplicated().sum() â†’ check duplicate rows



df.memory\_usage(deep=True) â†’ dataset size in memory



df\['col'].apply(type).value\_counts() â†’ check mixed data types



6\. Visualization for Exploration



df\['col'].hist() â†’ histogram



sns.boxplot(x=df\['col']) â†’ boxplot for outliers



sns.countplot(x='col', data=df) â†’ bar chart for categories



sns.scatterplot(x='Boxes', y='Amount', data=df) â†’ scatter for relationships



sns.pairplot(df) â†’ pairwise relationships



---

ðŸ§¹ Data Cleaning Techniques (20+)

1\. Handling Missing Data



df.isna().sum() â†’ check missing values per column



df.dropna() â†’ drop rows with missing values



df.fillna(value) â†’ fill missing with constant



df\['col'].fillna(df\['col'].mean()) â†’ fill with mean/median/mode



df.interpolate() â†’ fill missing using interpolation



df.ffill() / df.bfill() â†’ forward/backward fill



2\. Handling Duplicates



df.duplicated().sum() â†’ count duplicate rows



df.drop\_duplicates() â†’ remove duplicate rows



3\. Data Type Fixes



df\['col'] = df\['col'].astype(int) â†’ convert to integer



pd.to\_datetime(df\['date\_col']) â†’ convert to datetime



df\['col'] = df\['col'].astype('category') â†’ set categorical type



4\. Handling Outliers



sns.boxplot(x=df\['col']) â†’ visualize outliers



Z-score method â†’ remove rows where |z| > 3



IQR method â†’ keep only values within Q1â€“1.5IQR and Q3+1.5IQR



Winsorization â†’ cap extreme values



5\. Standardizing / Normalizing Data



(df\['col'] - df\['col'].min()) / (df\['col'].max() - df\['col'].min()) â†’ min-max scaling



(df\['col'] - df\['col'].mean()) / df\['col'].std() â†’ standardization (z-score)



6\. Fixing Inconsistencies



df\['col'].str.lower() / .str.upper() â†’ standardize text case



df\['col'].str.strip() â†’ remove leading/trailing spaces



df\['col'].replace({'hp': 'HP'}) â†’ unify categorical values



df.rename(columns={'old': 'new'}) â†’ fix column names



7\. Handling Invalid / Incorrect Data



Apply validation rules â†’ e.g. drop negative sales values



Cap unrealistic dates (e.g. future dates in past dataset)



Remove placeholder values (like "N/A", "?", "999")



8\. Encoding Categorical Data



pd.get\_dummies(df\['col']) â†’ one-hot encoding



df\['col'].astype('category').cat.codes â†’ label encoding



9\. Feature Cleaning



df\['col1'] + df\['col2'] â†’ fix or combine broken features



Extract useful parts (e.g. df\['date'].dt.year)



---

ðŸ”§ Data Wrangling Techniques (20+)

1\. Reshaping Data



df.melt() â†’ unpivot wide data into long format



df.pivot(index, columns, values) â†’ pivot long data into wide format



df.pivot\_table(values, index, columns, aggfunc='mean') â†’ summary pivot tables



df.stack() / df.unstack() â†’ reshape hierarchical columns



2\. Combining \& Merging Datasets



pd.concat(\[df1, df2]) â†’ combine DataFrames vertically or horizontally



pd.merge(df1, df2, on='col') â†’ SQL-style merge/join



df.join(other\_df) â†’ join on index



3\. Grouping \& Aggregation



df.groupby('col').sum() â†’ group by category and summarize



df.groupby(\['col1','col2']).mean() â†’ multi-key groupby



df.agg({'col1':'mean','col2':'sum'}) â†’ multiple aggregations



4\. Sorting \& Ranking



df.sort\_values(by='col') â†’ sort by column



df.sort\_values(by=\['col1','col2'], ascending=\[True,False]) â†’ multi-column sort



df\['rank'] = df\['col'].rank() â†’ rank values



5\. Index Operations



df.set\_index('col') â†’ set a column as index



df.reset\_index() â†’ reset index to default integers



df.sort\_index() â†’ sort by index



6\. Column Operations



df\['new'] = df\['col1'] + df\['col2'] â†’ create derived column



df.drop('col', axis=1) â†’ drop unwanted column



df.rename(columns={'old':'new'}) â†’ rename columns



df.eval('new = col1 + col2') â†’ create new columns efficiently



7\. Filtering \& Selecting Data



df\[df\['col'] > 100] â†’ filter rows by condition



df.query('col1 > 50 \& col2 == "HP"') â†’ SQL-like filtering



df.loc\[rows, cols] â†’ label-based selection



df.iloc\[rows, cols] â†’ integer-based selection



8\. Feature Engineering (light wrangling)



df\['date'].dt.year â†’ extract year



df\['text'].str.split().str\[0] â†’ extract part of text



df\['col'].map(lambda x: x\*2) â†’ apply transformation



---



ðŸ”„ Data Transformation Techniques (20+ Methods)

1\. Mathematical Transformations



df\['log\_col'] = np.log(df\['col']) â†’ log transform to reduce skewness



df\['sqrt\_col'] = np.sqrt(df\['col']) â†’ square root transform



df\['col'] = df\['col'] \*\* 2 â†’ power transformation



np.cbrt(df\['col']) â†’ cube root transform



np.reciprocal(df\['col']) â†’ reciprocal transform





2\. Scaling \& Normalization



StandardScaler().fit\_transform(df\[\['col']]) â†’ standardization (mean=0, std=1)



MinMaxScaler().fit\_transform(df\[\['col']]) â†’ normalize to \[0,1]



RobustScaler().fit\_transform(df\[\['col']]) â†’ scale ignoring outliers (uses IQR)



MaxAbsScaler().fit\_transform(df\[\['col']]) â†’ scale by maximum absolute value



Normalizer().fit\_transform(df\[\['col1','col2']]) â†’ row-wise normalization





**3. Encoding Categorical Variables**



* pd.get\_dummies(df, columns=\['col']) â†’ one-hot encoding



* LabelEncoder().fit\_transform(df\['col']) â†’ label encoding



* OrdinalEncoder().fit\_transform(df\[\['col']]) â†’ ordinal encoding



* Custom mapping â†’ df\['col'].map({'Low':1,'Medium':2,'High':3})



* Target encoding â†’ replace categories with mean target values





**4. Binning \& Discretization**



* pd.cut(df\['col'], bins=4) â†’ equal-width binning



* pd.qcut(df\['col'], q=4) â†’ quantile-based binning



* Manual binning â†’ df\['col'].apply(lambda x: 'High' if x>50 else 'Low')





**5. Date/Time Transformations**



* df\['date'].dt.year â†’ extract year



* df\['date'].dt.month â†’ extract month



* df\['date'].dt.day â†’ extract day



* df\['date'].dt.weekday â†’ extract weekday



* df\['date'].dt.is\_month\_end â†’ check if month end



* df\['date'].dt.to\_period('M') â†’ convert to monthly periods



**6. Text Transformations**



* df\['col'].str.lower() â†’ lowercase



* df\['col'].str.upper() â†’ uppercase



* df\['col'].str.strip() â†’ remove whitespace



* df\['col'].str.replace('old','new') â†’ replace text



* df\['col'].str.len() â†’ length of string



* Tokenization â†’ df\['col'].str.split()



7\. Feature Creation



* df\['total'] = df\['price'] \* df\['quantity'] â†’ new feature from existing columns



* df\['ratio'] = df\['num1'] / df\['num2'] â†’ ratio features



* Polynomial features â†’ PolynomialFeatures().fit\_transform(df\[\['col']])



* Interaction features â†’ df\['interaction'] = df\['col1'] \* df\['col2']



8\. Feature Selection / Reduction



df.corr() â†’ check correlation to drop redundant features



df.drop('col', axis=1) â†’ remove irrelevant feature



PCA(n\_components=2).fit\_transform(df) â†’ dimensionality reduction



---

