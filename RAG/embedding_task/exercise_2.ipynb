{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef5a09dd",
   "metadata": {},
   "source": [
    "Exercise 2 â€” Chunk Size Impact on Retrieval\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Chunk the same document into:\n",
    "\n",
    "   - 100-character chunks (small)\n",
    "\n",
    "   - 200-character chunks (medium)\n",
    "\n",
    "   - 400-character chunks (large)\n",
    "\n",
    "2. Create embeddings\n",
    "\n",
    "3. Search using the query: \"What is machine learning?\"\n",
    "\n",
    "4. Retrieve top 3 chunks\n",
    "\n",
    "5. Compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6330616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: f6ff49ca-5eee-4c52-b1b5-884c66d1b2a6)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Chunk Size Comparison:\n",
      "\n",
      "Small Chunks (100 chars):\n",
      "--------------------------------------------------\n",
      "- Number of chunks: 16\n",
      "- Top result (first 150 chars): \"ce of successfully achieving its goals.\n",
      "\n",
      "Machine learning is a subset of artificial intelligence tha\"\n",
      "- Score: 0.723\n",
      "- Analysis: Good precision but lacks full context.\n",
      "\n",
      "Medium Chunks (200 chars):\n",
      "--------------------------------------------------\n",
      "- Number of chunks: 8\n",
      "- Top result (first 150 chars): \"t focuses on the use of data\n",
      "and algorithms to imitate the way that humans learn, gradually improving its accuracy.\n",
      "Machine learning is an important c\"\n",
      "- Score: 0.704\n",
      "- Analysis: Best balance: enough detail + good focus.\n",
      "\n",
      "Large Chunks (400 chars):\n",
      "--------------------------------------------------\n",
      "- Number of chunks: 4\n",
      "- Top result (first 150 chars): \"t focuses on the use of data\n",
      "and algorithms to imitate the way that humans learn, gradually improving its accuracy.\n",
      "Machine learning is an important c\"\n",
      "- Score: 0.654\n",
      "- Analysis: More context but less focused; too broad.\n",
      "\n",
      "Best chunk size for this use case: 200 chars because it provides the best balance between focus and context.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Helper: cosine similarity\n",
    "def cosine_similarity(a, b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Helper: character-based chunking\n",
    "\n",
    "def chunk_by_char(text, size):\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), size):\n",
    "        chunk = text[i:i+size].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# document to be chunked and evaluated\n",
    "document = \"\"\"\n",
    "Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to\n",
    "the natural intelligence displayed by humans and animals. Leading AI textbooks define\n",
    "the field as the study of intelligent agents: any device that perceives its environment\n",
    "and takes actions that maximize its chance of successfully achieving its goals.\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that focuses on the use of data\n",
    "and algorithms to imitate the way that humans learn, gradually improving its accuracy.\n",
    "Machine learning is an important component of the growing field of data science.\n",
    "\n",
    "Deep learning is part of a broader family of machine learning methods based on artificial\n",
    "neural networks with representation learning. Learning can be supervised, semi-supervised\n",
    "or unsupervised. Deep learning architectures such as deep neural networks, deep belief\n",
    "networks, recurrent neural networks and convolutional neural networks have been applied\n",
    "to fields including computer vision, speech recognition, natural language processing,\n",
    "machine translation, and bioinformatics.\n",
    "\n",
    "Natural language processing is a subfield of linguistics, computer science, and artificial\n",
    "intelligence concerned with the interactions between computers and human language, in\n",
    "particular how to program computers to process and analyze large amounts of natural\n",
    "language data. Challenges in natural language processing frequently involve speech\n",
    "recognition, natural language understanding, and natural language generation.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "query = \"What is machine learning?\"\n",
    "query_emb = model.encode(query)\n",
    "\n",
    "# Function to evaluate chunk size\n",
    "def evaluate_chunk_size(label, chunk_size):\n",
    "    print(f\"\\n{label} Chunks ({chunk_size} chars):\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # 1. Chunk document\n",
    "    chunks = chunk_by_char(document, chunk_size)\n",
    "\n",
    "    # 2. Embed all chunks\n",
    "    chunk_embeddings = model.encode(chunks)\n",
    "\n",
    "    # 3. Compare to query\n",
    "    scores = []\n",
    "    for i, emb in enumerate(chunk_embeddings):\n",
    "        sim = cosine_similarity(query_emb, emb)\n",
    "        scores.append((chunks[i], sim))\n",
    "\n",
    "    # Sort by similarity\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    top = scores[:3]\n",
    "\n",
    "    # Print results\n",
    "    print(f\"- Number of chunks: {len(chunks)}\")\n",
    "    print(f\"- Top result (first 150 chars): \\\"{top[0][0][:150]}\\\"\")\n",
    "    print(f\"- Score: {top[0][1]:.3f}\")\n",
    "\n",
    "    # Analysis\n",
    "    if chunk_size == 100:\n",
    "        analysis = \"Good precision but lacks full context.\"\n",
    "    elif chunk_size == 200:\n",
    "        analysis = \"Best balance: enough detail + good focus.\"\n",
    "    else:\n",
    "        analysis = \"More context but less focused; too broad.\"\n",
    "\n",
    "    print(f\"- Analysis: {analysis}\")\n",
    "\n",
    "# Run evaluations\n",
    "\n",
    "print(\"\\n\\nChunk Size Comparison:\")\n",
    "\n",
    "evaluate_chunk_size(\"Small\", 100)\n",
    "evaluate_chunk_size(\"Medium\", 200)\n",
    "evaluate_chunk_size(\"Large\", 400)\n",
    "\n",
    "print(\"\\nBest chunk size for this use case: 200 chars because it provides the best balance between focus and context.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".allenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
