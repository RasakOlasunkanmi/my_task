{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba92faf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "936fc3ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004.06.11 07:18</td>\n",
       "      <td>384.00</td>\n",
       "      <td>384.10</td>\n",
       "      <td>384.00</td>\n",
       "      <td>384.00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004.06.11 07:23</td>\n",
       "      <td>384.10</td>\n",
       "      <td>384.10</td>\n",
       "      <td>384.00</td>\n",
       "      <td>384.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004.06.11 07:24</td>\n",
       "      <td>383.80</td>\n",
       "      <td>383.80</td>\n",
       "      <td>383.80</td>\n",
       "      <td>383.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004.06.11 07:25</td>\n",
       "      <td>383.80</td>\n",
       "      <td>384.30</td>\n",
       "      <td>383.80</td>\n",
       "      <td>384.30</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004.06.11 07:27</td>\n",
       "      <td>383.80</td>\n",
       "      <td>383.80</td>\n",
       "      <td>383.80</td>\n",
       "      <td>383.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6695755</th>\n",
       "      <td>2025.12.01 04:15</td>\n",
       "      <td>4248.22</td>\n",
       "      <td>4248.73</td>\n",
       "      <td>4247.18</td>\n",
       "      <td>4247.58</td>\n",
       "      <td>525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6695756</th>\n",
       "      <td>2025.12.01 04:16</td>\n",
       "      <td>4247.75</td>\n",
       "      <td>4248.30</td>\n",
       "      <td>4246.45</td>\n",
       "      <td>4246.59</td>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6695757</th>\n",
       "      <td>2025.12.01 04:17</td>\n",
       "      <td>4246.57</td>\n",
       "      <td>4248.02</td>\n",
       "      <td>4246.13</td>\n",
       "      <td>4246.26</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6695758</th>\n",
       "      <td>2025.12.01 04:18</td>\n",
       "      <td>4246.22</td>\n",
       "      <td>4247.23</td>\n",
       "      <td>4246.09</td>\n",
       "      <td>4246.44</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6695759</th>\n",
       "      <td>2025.12.01 04:19</td>\n",
       "      <td>4246.44</td>\n",
       "      <td>4246.73</td>\n",
       "      <td>4245.90</td>\n",
       "      <td>4246.58</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6695760 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Date     Open     High      Low    Close  Volume\n",
       "0        2004.06.11 07:18   384.00   384.10   384.00   384.00       3\n",
       "1        2004.06.11 07:23   384.10   384.10   384.00   384.00       2\n",
       "2        2004.06.11 07:24   383.80   383.80   383.80   383.80       1\n",
       "3        2004.06.11 07:25   383.80   384.30   383.80   384.30       3\n",
       "4        2004.06.11 07:27   383.80   383.80   383.80   383.80       1\n",
       "...                   ...      ...      ...      ...      ...     ...\n",
       "6695755  2025.12.01 04:15  4248.22  4248.73  4247.18  4247.58     525\n",
       "6695756  2025.12.01 04:16  4247.75  4248.30  4246.45  4246.59     391\n",
       "6695757  2025.12.01 04:17  4246.57  4248.02  4246.13  4246.26     339\n",
       "6695758  2025.12.01 04:18  4246.22  4247.23  4246.09  4246.44     299\n",
       "6695759  2025.12.01 04:19  4246.44  4246.73  4245.90  4246.58     169\n",
       "\n",
       "[6695760 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"XAU_1m_data.csv\", sep=\";\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e9f61b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date      0\n",
       "Open      0\n",
       "High      0\n",
       "Low       0\n",
       "Close     0\n",
       "Volume    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a55f6710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17a2d6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6695760</td>\n",
       "      <td>6.695760e+06</td>\n",
       "      <td>6.695760e+06</td>\n",
       "      <td>6.695760e+06</td>\n",
       "      <td>6.695760e+06</td>\n",
       "      <td>6.695760e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>6695760</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2004.06.11 07:18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.457690e+03</td>\n",
       "      <td>1.457906e+03</td>\n",
       "      <td>1.457471e+03</td>\n",
       "      <td>1.457689e+03</td>\n",
       "      <td>6.307870e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.616410e+02</td>\n",
       "      <td>5.617312e+02</td>\n",
       "      <td>5.615506e+02</td>\n",
       "      <td>5.616422e+02</td>\n",
       "      <td>7.978339e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.811000e+02</td>\n",
       "      <td>3.811000e+02</td>\n",
       "      <td>3.811000e+02</td>\n",
       "      <td>3.811000e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.179090e+03</td>\n",
       "      <td>1.179260e+03</td>\n",
       "      <td>1.178930e+03</td>\n",
       "      <td>1.179090e+03</td>\n",
       "      <td>1.600000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.324620e+03</td>\n",
       "      <td>1.324800e+03</td>\n",
       "      <td>1.324450e+03</td>\n",
       "      <td>1.324620e+03</td>\n",
       "      <td>4.400000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.772140e+03</td>\n",
       "      <td>1.772410e+03</td>\n",
       "      <td>1.771890e+03</td>\n",
       "      <td>1.772140e+03</td>\n",
       "      <td>8.200000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.254980e+03</td>\n",
       "      <td>4.256340e+03</td>\n",
       "      <td>4.252710e+03</td>\n",
       "      <td>4.255040e+03</td>\n",
       "      <td>5.959000e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Date          Open          High           Low  \\\n",
       "count            6695760  6.695760e+06  6.695760e+06  6.695760e+06   \n",
       "unique           6695760           NaN           NaN           NaN   \n",
       "top     2004.06.11 07:18           NaN           NaN           NaN   \n",
       "freq                   1           NaN           NaN           NaN   \n",
       "mean                 NaN  1.457690e+03  1.457906e+03  1.457471e+03   \n",
       "std                  NaN  5.616410e+02  5.617312e+02  5.615506e+02   \n",
       "min                  NaN  3.811000e+02  3.811000e+02  3.811000e+02   \n",
       "25%                  NaN  1.179090e+03  1.179260e+03  1.178930e+03   \n",
       "50%                  NaN  1.324620e+03  1.324800e+03  1.324450e+03   \n",
       "75%                  NaN  1.772140e+03  1.772410e+03  1.771890e+03   \n",
       "max                  NaN  4.254980e+03  4.256340e+03  4.252710e+03   \n",
       "\n",
       "               Close        Volume  \n",
       "count   6.695760e+06  6.695760e+06  \n",
       "unique           NaN           NaN  \n",
       "top              NaN           NaN  \n",
       "freq             NaN           NaN  \n",
       "mean    1.457689e+03  6.307870e+01  \n",
       "std     5.616422e+02  7.978339e+01  \n",
       "min     3.811000e+02  1.000000e+00  \n",
       "25%     1.179090e+03  1.600000e+01  \n",
       "50%     1.324620e+03  4.400000e+01  \n",
       "75%     1.772140e+03  8.200000e+01  \n",
       "max     4.255040e+03  5.959000e+03  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "417132aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f9e65_row0_col0, #T_f9e65_row0_col7, #T_f9e65_row1_col0, #T_f9e65_row1_col7, #T_f9e65_row2_col0, #T_f9e65_row2_col7, #T_f9e65_row3_col0, #T_f9e65_row3_col7, #T_f9e65_row4_col0, #T_f9e65_row4_col1, #T_f9e65_row4_col2, #T_f9e65_row4_col3, #T_f9e65_row4_col4, #T_f9e65_row4_col5, #T_f9e65_row4_col6 {\n",
       "  background-color: #fff7fb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_f9e65_row0_col1, #T_f9e65_row0_col2, #T_f9e65_row0_col3, #T_f9e65_row0_col4, #T_f9e65_row0_col5, #T_f9e65_row0_col6, #T_f9e65_row1_col1, #T_f9e65_row1_col2, #T_f9e65_row1_col3, #T_f9e65_row1_col4, #T_f9e65_row1_col5, #T_f9e65_row1_col6, #T_f9e65_row2_col1, #T_f9e65_row2_col2, #T_f9e65_row2_col3, #T_f9e65_row2_col4, #T_f9e65_row2_col5, #T_f9e65_row2_col6, #T_f9e65_row3_col1, #T_f9e65_row3_col2, #T_f9e65_row3_col3, #T_f9e65_row3_col4, #T_f9e65_row3_col5, #T_f9e65_row3_col6, #T_f9e65_row4_col7 {\n",
       "  background-color: #023858;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f9e65\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f9e65_level0_col0\" class=\"col_heading level0 col0\" >count</th>\n",
       "      <th id=\"T_f9e65_level0_col1\" class=\"col_heading level0 col1\" >mean</th>\n",
       "      <th id=\"T_f9e65_level0_col2\" class=\"col_heading level0 col2\" >std</th>\n",
       "      <th id=\"T_f9e65_level0_col3\" class=\"col_heading level0 col3\" >min</th>\n",
       "      <th id=\"T_f9e65_level0_col4\" class=\"col_heading level0 col4\" >25%</th>\n",
       "      <th id=\"T_f9e65_level0_col5\" class=\"col_heading level0 col5\" >50%</th>\n",
       "      <th id=\"T_f9e65_level0_col6\" class=\"col_heading level0 col6\" >75%</th>\n",
       "      <th id=\"T_f9e65_level0_col7\" class=\"col_heading level0 col7\" >max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f9e65_level0_row0\" class=\"row_heading level0 row0\" >Open</th>\n",
       "      <td id=\"T_f9e65_row0_col0\" class=\"data row0 col0\" >6695760.000000</td>\n",
       "      <td id=\"T_f9e65_row0_col1\" class=\"data row0 col1\" >1457.689686</td>\n",
       "      <td id=\"T_f9e65_row0_col2\" class=\"data row0 col2\" >561.640971</td>\n",
       "      <td id=\"T_f9e65_row0_col3\" class=\"data row0 col3\" >381.100000</td>\n",
       "      <td id=\"T_f9e65_row0_col4\" class=\"data row0 col4\" >1179.090000</td>\n",
       "      <td id=\"T_f9e65_row0_col5\" class=\"data row0 col5\" >1324.620000</td>\n",
       "      <td id=\"T_f9e65_row0_col6\" class=\"data row0 col6\" >1772.140000</td>\n",
       "      <td id=\"T_f9e65_row0_col7\" class=\"data row0 col7\" >4254.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9e65_level0_row1\" class=\"row_heading level0 row1\" >High</th>\n",
       "      <td id=\"T_f9e65_row1_col0\" class=\"data row1 col0\" >6695760.000000</td>\n",
       "      <td id=\"T_f9e65_row1_col1\" class=\"data row1 col1\" >1457.906337</td>\n",
       "      <td id=\"T_f9e65_row1_col2\" class=\"data row1 col2\" >561.731238</td>\n",
       "      <td id=\"T_f9e65_row1_col3\" class=\"data row1 col3\" >381.100000</td>\n",
       "      <td id=\"T_f9e65_row1_col4\" class=\"data row1 col4\" >1179.260000</td>\n",
       "      <td id=\"T_f9e65_row1_col5\" class=\"data row1 col5\" >1324.800000</td>\n",
       "      <td id=\"T_f9e65_row1_col6\" class=\"data row1 col6\" >1772.410000</td>\n",
       "      <td id=\"T_f9e65_row1_col7\" class=\"data row1 col7\" >4256.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9e65_level0_row2\" class=\"row_heading level0 row2\" >Low</th>\n",
       "      <td id=\"T_f9e65_row2_col0\" class=\"data row2 col0\" >6695760.000000</td>\n",
       "      <td id=\"T_f9e65_row2_col1\" class=\"data row2 col1\" >1457.470983</td>\n",
       "      <td id=\"T_f9e65_row2_col2\" class=\"data row2 col2\" >561.550588</td>\n",
       "      <td id=\"T_f9e65_row2_col3\" class=\"data row2 col3\" >381.100000</td>\n",
       "      <td id=\"T_f9e65_row2_col4\" class=\"data row2 col4\" >1178.930000</td>\n",
       "      <td id=\"T_f9e65_row2_col5\" class=\"data row2 col5\" >1324.450000</td>\n",
       "      <td id=\"T_f9e65_row2_col6\" class=\"data row2 col6\" >1771.890000</td>\n",
       "      <td id=\"T_f9e65_row2_col7\" class=\"data row2 col7\" >4252.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9e65_level0_row3\" class=\"row_heading level0 row3\" >Close</th>\n",
       "      <td id=\"T_f9e65_row3_col0\" class=\"data row3 col0\" >6695760.000000</td>\n",
       "      <td id=\"T_f9e65_row3_col1\" class=\"data row3 col1\" >1457.689445</td>\n",
       "      <td id=\"T_f9e65_row3_col2\" class=\"data row3 col2\" >561.642211</td>\n",
       "      <td id=\"T_f9e65_row3_col3\" class=\"data row3 col3\" >381.100000</td>\n",
       "      <td id=\"T_f9e65_row3_col4\" class=\"data row3 col4\" >1179.090000</td>\n",
       "      <td id=\"T_f9e65_row3_col5\" class=\"data row3 col5\" >1324.620000</td>\n",
       "      <td id=\"T_f9e65_row3_col6\" class=\"data row3 col6\" >1772.140000</td>\n",
       "      <td id=\"T_f9e65_row3_col7\" class=\"data row3 col7\" >4255.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f9e65_level0_row4\" class=\"row_heading level0 row4\" >Volume</th>\n",
       "      <td id=\"T_f9e65_row4_col0\" class=\"data row4 col0\" >6695760.000000</td>\n",
       "      <td id=\"T_f9e65_row4_col1\" class=\"data row4 col1\" >63.078697</td>\n",
       "      <td id=\"T_f9e65_row4_col2\" class=\"data row4 col2\" >79.783390</td>\n",
       "      <td id=\"T_f9e65_row4_col3\" class=\"data row4 col3\" >1.000000</td>\n",
       "      <td id=\"T_f9e65_row4_col4\" class=\"data row4 col4\" >16.000000</td>\n",
       "      <td id=\"T_f9e65_row4_col5\" class=\"data row4 col5\" >44.000000</td>\n",
       "      <td id=\"T_f9e65_row4_col6\" class=\"data row4 col6\" >82.000000</td>\n",
       "      <td id=\"T_f9e65_row4_col7\" class=\"data row4 col7\" >5959.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x230e54edd00>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T.style.background_gradient(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c32739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# GENERAL EDA - XAUUSD (2004-2024)\n",
    "# Complete Step-by-Step Analysis Without Functions or Abstractions\n",
    "# Purpose: Deep understanding of data before modeling\n",
    "# ================================================================================\n",
    "\n",
    "# ================================================================================\n",
    "# SECTION 0: LIBRARY IMPORTS\n",
    "# ================================================================================\n",
    "# We import libraries upfront. These are tools we'll use throughout the notebook.\n",
    "# Each import is explained so you understand what it does.\n",
    "\n",
    "import pandas as pd                    # For data manipulation and analysis\n",
    "import numpy as np                     # For numerical operations and array handling\n",
    "import matplotlib.pyplot as plt        # For creating visualizations (plots, charts)\n",
    "import seaborn as sns                  # For statistical visualizations (prettier than matplotlib)\n",
    "from datetime import datetime, timedelta  # For working with dates and time\n",
    "import warnings                        # To suppress warning messages\n",
    "warnings.filterwarnings('ignore')      # Suppress warnings for cleaner output\n",
    "\n",
    "# Statistical libraries for analysis\n",
    "from scipy import stats                # For statistical tests (mean, std, distributions)\n",
    "from scipy.stats import skew, kurtosis  # For measuring distribution shape\n",
    "from statsmodels.tsa.stattools import adfuller  # For stationarity testing\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf  # For autocorrelation\n",
    "\n",
    "# Set display options for better readability\n",
    "pd.set_option('display.max_columns', None)  # Show all columns when printing dataframes\n",
    "pd.set_option('display.max_rows', 100)      # Show max 100 rows when printing\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.4f}')  # Show 4 decimal places\n",
    "\n",
    "# Set visual style for plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')  # Dark background with grid\n",
    "sns.set_palette(\"husl\")                  # Color palette for plots\n",
    "plt.rcParams['figure.figsize'] = (14, 6) # Default figure size\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GENERAL EDA - XAUUSD (2004-2024)\")\n",
    "print(\"Step-by-Step Analysis Without Functions\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ================================================================================\n",
    "# SECTION 1: LOAD DATA\n",
    "# ================================================================================\n",
    "# Load the CSV file from MT4/MT5 export\n",
    "# The file should be in the same directory as this notebook or specify the path\n",
    "\n",
    "print(\"\\n[SECTION 1] LOADING DATA FROM CSV...\")\n",
    "\n",
    "# Read the CSV file\n",
    "# We use pd.read_csv() to load the data into a DataFrame\n",
    "# A DataFrame is like a spreadsheet with rows and columns\n",
    "df = pd.read_csv('xauusd_2004_2024.csv')\n",
    "\n",
    "print(f\"✓ Data loaded successfully!\")\n",
    "print(f\"  Total rows: {len(df):,}\")\n",
    "print(f\"  Total columns: {df.shape[1]}\")\n",
    "print(f\"  Columns: {list(df.columns)}\")\n",
    "\n",
    "# Show first few rows to understand the structure\n",
    "print(\"\\nFirst 5 rows of data:\")\n",
    "print(df.head())\n",
    "\n",
    "# Show data types of each column\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# ================================================================================\n",
    "# SECTION 2: PARSE DATETIME AND DETECT TIMEZONE\n",
    "# ================================================================================\n",
    "# MT4 exports dates in format \"2004.06.11 07:18\" (YYYY.MM.DD HH:MM)\n",
    "# We need to convert this text into proper datetime objects that Python understands\n",
    "# Then we check if the times are UTC or broker-adjusted\n",
    "\n",
    "print(\"\\n[SECTION 2] PARSING DATETIME AND CHECKING TIMEZONE...\")\n",
    "\n",
    "# Convert the Date column from text to datetime objects\n",
    "# The format parameter tells pandas how to interpret the date string\n",
    "# %Y = 4-digit year (2004)\n",
    "# %m = 2-digit month (06)\n",
    "# %d = 2-digit day (11)\n",
    "# %H = 2-digit hour (07)\n",
    "# %M = 2-digit minute (18)\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y.%m.%d %H:%M')\n",
    "\n",
    "print(f\"✓ DateTime parsed successfully!\")\n",
    "print(f\"  First timestamp: {df['Date'].iloc[0]}\")\n",
    "print(f\"  Last timestamp: {df['Date'].iloc[-1]}\")\n",
    "\n",
    "# Set Date as index (this makes it the row identifier)\n",
    "# This is useful for time-series analysis\n",
    "df.set_index('Date', inplace=True)\n",
    "df.index.name = 'DateTime'\n",
    "\n",
    "print(f\"\\nDataFrame index is now DateTime (first 5):\")\n",
    "print(df.index[:5])\n",
    "\n",
    "# Now check timezone: Is this UTC or broker-adjusted time?\n",
    "# XAUUSD trades 24 hours on forex (Sunday 5PM EST to Friday 5PM EST)\n",
    "# If we see trading activity in all 24 hours, it's likely UTC\n",
    "# If we see gaps at specific hours, it might be broker time\n",
    "\n",
    "print(\"\\n--- TIMEZONE DETECTION ---\")\n",
    "print(\"\\nAnalyzing which hours have trading activity...\")\n",
    "\n",
    "# Extract the hour from each timestamp\n",
    "# For example: 2004-06-11 07:18 → hour = 7\n",
    "hours_in_data = df.index.hour\n",
    "\n",
    "# Count how many times each hour appears\n",
    "hour_counts = hours_in_data.value_counts().sort_index()\n",
    "\n",
    "print(\"\\nHour distribution (showing how many candles in each hour):\")\n",
    "for hour in range(24):\n",
    "    count = hour_counts.get(hour, 0)\n",
    "    # Create a visual bar to show distribution\n",
    "    bar = '█' * (count // 1000)  # Each block = ~1000 candles\n",
    "    print(f\"  Hour {hour:02d}:00 - {count:8,} candles {bar}\")\n",
    "\n",
    "# Check if we have activity in all 24 hours\n",
    "unique_hours = len(hour_counts)\n",
    "print(f\"\\nUnique hours with data: {unique_hours}/24\")\n",
    "\n",
    "# XAUUSD typically trades all 24 hours on forex\n",
    "# If we see all 24 hours, it's UTC-based\n",
    "if unique_hours == 24:\n",
    "    print(\"✓ Likely UTC timezone (trading active all 24 hours)\")\n",
    "    detected_timezone = \"UTC\"\n",
    "else:\n",
    "    print(\"⚠ Possible broker adjustment or limited trading hours\")\n",
    "    detected_timezone = \"Broker-adjusted\"\n",
    "\n",
    "print(f\"\\nTimezone: {detected_timezone}\")\n",
    "\n",
    "# ================================================================================\n",
    "# SECTION 3: CHECK BASIC DATA STRUCTURE\n",
    "# ================================================================================\n",
    "# Before we analyze the data, we need to make sure the data is valid\n",
    "# We check for missing values, duplicates, and data integrity\n",
    "\n",
    "print(\"\\n[SECTION 3] CHECKING DATA STRUCTURE AND INTEGRITY...\")\n",
    "\n",
    "# Check for missing values (NaN, None, empty cells)\n",
    "print(\"\\nMissing values per column:\")\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_pct = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "for column in df.columns:\n",
    "    missing = missing_counts[column]\n",
    "    pct = missing_pct[column]\n",
    "    if missing > 0:\n",
    "        print(f\"  {column}: {missing:,} missing ({pct:.4f}%)\")\n",
    "    else:\n",
    "        print(f\"  {column}: 0 missing (0.00%) ✓\")\n",
    "\n",
    "# If Close has missing values, we need to fill them\n",
    "# We use interpolation (linear interpolation fills gaps smoothly)\n",
    "if df['Close'].isnull().sum() > 0:\n",
    "    print(f\"\\n⚠ Found {df['Close'].isnull().sum()} missing Close values\")\n",
    "    print(\"  Applying linear interpolation to fill missing values...\")\n",
    "    df['Close'] = df['Close'].interpolate(method='linear')\n",
    "    print(\"  ✓ Interpolation complete\")\n",
    "\n",
    "# Check for duplicate timestamps\n",
    "# Each timestamp should appear only once (one 1-minute candle per minute)\n",
    "print(f\"\\nDuplicate timestamps: {df.index.duplicated().sum()}\")\n",
    "\n",
    "if df.index.duplicated().sum() > 0:\n",
    "    print(f\"  Removing {df.index.duplicated().sum()} duplicate rows...\")\n",
    "    df = df[~df.index.duplicated(keep='first')]\n",
    "    print(f\"  ✓ Duplicates removed. New shape: {df.shape[0]:,} rows\")\n",
    "\n",
    "# Check data integrity: OHLC relationships\n",
    "# High must be >= Open and Close (logically, high is the highest price)\n",
    "# Low must be <= Open and Close (low is the lowest price)\n",
    "# High must be >= Low (always)\n",
    "\n",
    "print(\"\\nOHLC Integrity Checks:\")\n",
    "\n",
    "# Check 1: High >= Open AND High >= Close\n",
    "high_check = (df['High'] >= df['Open']) & (df['High'] >= df['Close'])\n",
    "high_valid = high_check.sum()\n",
    "print(f\"  High >= Open & Close: {high_valid:,}/{len(df):,} ({(high_valid/len(df)*100):.2f}%) ✓\")\n",
    "\n",
    "# Check 2: Low <= Open AND Low <= Close\n",
    "low_check = (df['Low'] <= df['Open']) & (df['Low'] <= df['Close'])\n",
    "low_valid = low_check.sum()\n",
    "print(f\"  Low <= Open & Close: {low_valid:,}/{len(df):,} ({(low_valid/len(df)*100):.2f}%) ✓\")\n",
    "\n",
    "# Check 3: High >= Low (fundamental check)\n",
    "hl_check = df['High'] >= df['Low']\n",
    "hl_valid = hl_check.sum()\n",
    "print(f\"  High >= Low: {hl_valid:,}/{len(df):,} ({(hl_valid/len(df)*100):.2f}%) ✓\")\n",
    "\n",
    "# Mark any rows with integrity issues for inspection\n",
    "invalid_rows = (~high_check) | (~low_check) | (~hl_check)\n",
    "num_invalid = invalid_rows.sum()\n",
    "\n",
    "if num_invalid > 0:\n",
    "    print(f\"\\n⚠ Warning: {num_invalid} rows have OHLC integrity issues\")\n",
    "    print(\"  These will be flagged but not removed (for investigation)\")\n",
    "else:\n",
    "    print(\"\\n✓ All OHLC relationships are valid!\")\n",
    "\n",
    "# ================================================================================\n",
    "# SECTION 4: TIME-SERIES STRUCTURE - GAPS AND MISSING CANDLES\n",
    "# ================================================================================\n",
    "# Check if there are gaps in the time series\n",
    "# Gaps would indicate missing candles (e.g., market closed on weekends)\n",
    "# We expect gaps on weekends and holidays\n",
    "\n",
    "print(\"\\n[SECTION 4] ANALYZING TIME-SERIES GAPS...\")\n",
    "\n",
    "# Calculate the time difference between consecutive candles\n",
    "# In a normal situation, this should be 1 minute\n",
    "time_differences = df.index.to_series().diff()\n",
    "\n",
    "# Find all gaps larger than 1 minute\n",
    "# dropna() removes the first row (which has NaN difference)\n",
    "gaps = time_differences[time_differences > timedelta(minutes=1)].dropna()\n",
    "\n",
    "print(f\"\\nTime gaps detected: {len(gaps):,}\")\n",
    "print(f\"  Average gap: {gaps.mean()}\")\n",
    "print(f\"  Largest gap: {gaps.max()}\")\n",
    "print(f\"  Smallest gap: {gaps.min()}\")\n",
    "\n",
    "# Analyze which days of the week have gaps\n",
    "# Days: 0=Monday, 1=Tuesday, ..., 6=Sunday\n",
    "gap_day_of_week = gaps.index.dayofweek\n",
    "gap_distribution = gap_day_of_week.value_counts().sort_index()\n",
    "\n",
    "day_names = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday',\n",
    "             4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
    "\n",
    "print(f\"\\nGaps by day of week:\")\n",
    "for day_num in range(7):\n",
    "    count = gap_distribution.get(day_num, 0)\n",
    "    print(f\"  {day_names[day_num]}: {count:,} gaps\")\n",
    "\n",
    "print(f\"\\n✓ Gap analysis complete (expected: gaps on weekends)\")\n",
    "\n",
    "# ================================================================================\n",
    "# SECTION 5: BASIC STATISTICS - UNIVARIATE ANALYSIS PART 1\n",
    "# ================================================================================\n",
    "# Now we dive into analyzing individual variables\n",
    "# We start with basic statistics: mean, std, min, max, etc.\n",
    "\n",
    "print(\"\\n[SECTION 5] BASIC STATISTICS (UNIVARIATE ANALYSIS)...\")\n",
    "\n",
    "print(\"\\n--- OPEN PRICES ---\")\n",
    "print(df['Open'].describe())\n",
    "\n",
    "print(\"\\n--- HIGH PRICES ---\")\n",
    "print(df['High'].describe())\n",
    "\n",
    "print(\"\\n--- LOW PRICES ---\")\n",
    "print(df['Low'].describe())\n",
    "\n",
    "print(\"\\n--- CLOSE PRICES ---\")\n",
    "print(df['Close'].describe())\n",
    "\n",
    "print(\"\\n--- VOLUME ---\")\n",
    "print(df['Volume'].describe())\n",
    "\n",
    "# Calculate additional statistics manually to understand them better\n",
    "print(\"\\n--- ADDITIONAL STATISTICS FOR CLOSE PRICES ---\")\n",
    "\n",
    "close_prices = df['Close']\n",
    "\n",
    "# Mean: average price\n",
    "mean_close = close_prices.mean()\n",
    "print(f\"Mean (average): ${mean_close:.2f}\")\n",
    "\n",
    "# Median: middle value when sorted\n",
    "median_close = close_prices.median()\n",
    "print(f\"Median (middle): ${median_close:.2f}\")\n",
    "\n",
    "# Mode: most common value\n",
    "mode_close = close_prices.mode()[0] if len(close_prices.mode()) > 0 else None\n",
    "print(f\"Mode (most common): ${mode_close:.2f}\" if mode_close else \"Mode: Not available\")\n",
    "\n",
    "# Standard deviation: how spread out the prices are\n",
    "std_close = close_prices.std()\n",
    "print(f\"Std Dev (spread): ${std_close:.2f}\")\n",
    "\n",
    "# Variance: square of standard deviation\n",
    "var_close = close_prices.var()\n",
    "print(f\"Variance: {var_close:.2f}\")\n",
    "\n",
    "# Range: difference between max and min\n",
    "range_close = close_prices.max() - close_prices.min()\n",
    "print(f\"Range (max - min): ${range_close:.2f}\")\n",
    "\n",
    "# Coefficient of Variation: std / mean (shows relative volatility)\n",
    "cv_close = (std_close / mean_close) * 100\n",
    "print(f\"Coefficient of Variation: {cv_close:.2f}%\")\n",
    "\n",
    "# ================================================================================\n",
    "# SECTION 6: CALCULATE RETURNS\n",
    "# ================================================================================\n",
    "# Returns are more important than prices for trading\n",
    "# A return shows the percentage change from one period to the next\n",
    "# Returns are stationary (mean-reverting), while prices are not\n",
    "\n",
    "print(\"\\n[SECTION 6] CALCULATING RETURNS...\")\n",
    "\n",
    "# Simple return: (Price_today - Price_yesterday) / Price_yesterday\n",
    "# This shows the percentage change\n",
    "df['Simple_Return'] = df['Close'].pct_change()\n",
    "\n",
    "# Log return: ln(Price_today / Price_yesterday)\n",
    "# This is preferred for financial data (more mathematically stable)\n",
    "df['Log_Return'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "\n",
    "print(f\"\\nSimple Return statistics:\")\n",
    "print(f\"  Mean: {df['Simple_Return'].mean():.6f}\")\n",
    "print(f\"  Std Dev: {df['Simple_Return'].std():.6f}\")\n",
    "print(f\"  Min: {df['Simple_Return'].min():.6f}\")\n",
    "print(f\"  Max: {df['Simple_Return'].max():.6f}\")\n",
    "\n",
    "print(f\"\\nLog Return statistics:\")\n",
    "print(f\"  Mean: {df['Log_Return'].mean():.6f}\")\n",
    "print(f\"  Std Dev: {df['Log_Return'].std():.6f}\")\n",
    "print(f\"  Min: {df['Log_Return'].min():.6f}\")\n",
    "print(f\"  Max: {df['Log_Return'].max():.6f}\")\n",
    "\n",
    "# Calculate additional statistics for returns\n",
    "print(f\"\\nReturn Distribution Analysis:\")\n",
    "\n",
    "# Skewness: is the distribution skewed to the left or right?\n",
    "# Positive skew = tail on right (more extreme high values)\n",
    "# Negative skew = tail on left (more extreme low values)\n",
    "skewness = skew(df['Log_Return'].dropna())\n",
    "print(f\"  Skewness: {skewness:.4f} (negative = left tail, positive = right tail)\")\n",
    "\n",
    "# Kurtosis: how fat are the tails?\n",
    "# Normal distribution = 0 (excess kurtosis)\n",
    "# Fat tails (extreme events common) = positive kurtosis\n",
    "# Thin tails = negative kurtosis\n",
    "kurt = kurtosis(df['Log_Return'].dropna())\n",
    "print(f\"  Kurtosis: {kurt:.4f} (positive = fat tails, extreme events common)\")\n",
    "\n",
    "# ================================================================================\n",
    "# SECTION 7: STATIONARITY TESTING\n",
    "# ================================================================================\n",
    "# Stationarity is important for time-series modeling\n",
    "# Stationary series have constant mean and variance over time\n",
    "# Non-stationary series trend upward or downward\n",
    "\n",
    "print(\"\\n[SECTION 7] STATIONARITY TESTING (ADF TEST)...\")\n",
    "\n",
    "# Test 1: Close prices (should be non-stationary)\n",
    "print(\"\\nTesting Close prices (raw prices)...\")\n",
    "adf_close = adfuller(df['Close'].dropna(), autolag='AIC')\n",
    "print(f\"  ADF Statistic: {adf_close[0]:.6f}\")\n",
    "print(f\"  P-value: {adf_close[1]:.6f}\")\n",
    "\n",
    "if adf_close[1] < 0.05:\n",
    "    print(f\"  Result: STATIONARY (p < 0.05) - Reject null hypothesis\")\n",
    "else:\n",
    "    print(f\"  Result: NON-STATIONARY (p >= 0.05) - Fail to reject null hypothesis\")\n",
    "\n",
    "# Test 2: Log returns (should be stationary)\n",
    "print(\"\\nTesting Log Returns...\")\n",
    "adf_returns = adfuller(df['Log_Return'].dropna(), autolag='AIC')\n",
    "print(f\"  ADF Statistic: {adf_returns[0]:.6f}\")\n",
    "print(f\"  P-value: {adf_returns[1]:.6f}\")\n",
    "\n",
    "if adf_returns[1] < 0.05:\n",
    "    print(f\"  Result: STATIONARY (p < 0.05) - Good for modeling!\")\n",
    "else:\n",
    "    print(f\"  Result: NON-STATIONARY (p >= 0.05) - Might need differencing\")\n",
    "\n",
    "print(\"\\n✓ Interpretation:\")\n",
    "print(\"  - Close prices should be non-stationary (they trend over time)\")\n",
    "print(\"  - Returns should be stationary (mean-reverting property)\")\n",
    "\n",
    "# ================================================================================\n",
    "# SECTION 8: CALCULATE ROLLING VOLATILITY\n",
    "# ================================================================================\n",
    "# Volatility is how much the price moves\n",
    "# We calculate rolling volatility to see how it changes over time\n",
    "# Higher volatility = bigger price swings = more risk and opportunity\n",
    "\n",
    "print(\"\\n[SECTION 8] CALCULATING ROLLING VOLATILITY...\")\n",
    "\n",
    "# Rolling standard deviation of returns (window = number of periods)\n",
    "# 30-minute window: std of returns over last 30 minutes\n",
    "df['Volatility_30m'] = df['Log_Return'].rolling(window=30).std()\n",
    "\n",
    "# 60-minute window: std of returns over last 60 minutes\n",
    "df['Volatility_60m'] = df['Log_Return'].rolling(window=60).std()\n",
    "\n",
    "# 1440-minute window: std of returns over last 24 hours (daily volatility)\n",
    "df['Volatility_1d'] = df['Log_Return'].rolling(window=1440).std()\n",
    "\n",
    "print(f\"\\n30-minute volatility: {df['Volatility_30m'].describe()}\")\n",
    "print(f\"\\n60-minute volatility: {df['Volatility_60m'].describe()}\")\n",
    "print(f\"\\n1-day volatility: {df['Volatility_1d'].describe()}\")\n",
    "\n",
    "# Annualized volatility (if we want annual perspective)\n",
    "# Volatility is typically expressed as annual standard deviation\n",
    "# Daily volatility * sqrt(252 trading days) ≈ annual volatility\n",
    "# Minute volatility * sqrt(525600 minutes in a year) ≈ annual volatility\n",
    "\n",
    "annual_volatility = df['Volatility_1d'].mean() * np.sqrt(252)\n",
    "print(f\"\\nAnnualized volatility (daily rolling): {annual_volatility:.4f} or {annual_volatility*100:.2f}%\")\n",
    "\n",
    "# ================================================================================\n",
    "# SECTION 9: IDENTIFY MARKET REGIMES (VOLATILITY-BASED)\n",
    "# ================================================================================\n",
    "# Markets behave differently in different regimes\n",
    "# High volatility regime: risky, fast movements\n",
    "# Low volatility regime: stable, slow movements\n",
    "# We'll classify each period into regimes based on volatility\n",
    "\n",
    "print(\"\\n[SECTION 9] IDENTIFYING MARKET REGIMES...\")\n",
    "\n",
    "# Use the 1-day rolling volatility to classify regimes\n",
    "# We calculate percentiles to define regime thresholds\n",
    "vol_1d = df['Volatility_1d'].dropna()\n",
    "\n",
    "# Calculate percentiles\n",
    "vol_33_percentile = vol_1d.quantile(0.33)  # Lower third\n",
    "vol_67_percentile = vol_1d.quantile(0.67)  # Upper third\n",
    "\n",
    "print(f\"\\nVolatility Percentiles (for regime definition):\")\n",
    "print(f\"  33rd percentile (low vol boundary): {vol_33_percentile:.6f}\")\n",
    "print(f\"  67th percentile (high vol boundary): {vol_67_percentile:.6f}\")\n",
    "\n",
    "# Classify each candle into a regime\n",
    "# 0 = Low volatility\n",
    "# 1 = Medium volatility\n",
    "# 2 = High volatility\n",
    "df['Regime'] = 1  # Default to medium\n",
    "\n",
    "# Low volatility: below 33rd percentile\n",
    "df.loc[df['Volatility_1d'] <= vol_33_percentile, 'Regime'] = 0\n",
    "\n",
    "# High volatility: above 67th percentile\n",
    "df.loc[df['Volatility_1d'] > vol_67_percentile, 'Regime'] = 2\n",
    "\n",
    "print(f\"\\nRegime Distribution:\")\n",
    "regime_counts = df['Regime'].value_counts().sort_index()\n",
    "for regime, count in regime_counts.items():\n",
    "    pct = (count / len(df)) * 100\n",
    "    regime_name = ['LOW', 'MEDIUM', 'HIGH'][regime]\n",
    "    print(f\"  Regime {regime} ({regime_name:6}): {count:8,} candles ({pct:5.2f}%)\")\n",
    "\n",
    "# ================================================================================\n",
    "# SECTION 10: VOLUME ANALYSIS - CHECKING USEFULNESS\n",
    "# ================================================================================\n",
    "# Volume tells us how much trading happened\n",
    "# In forex, volume is often just tick count, not true trading volume\n",
    "# We need to check if volume provides useful information\n",
    "\n",
    "print(\"\\n[SECTION 10] VOLUME ANALYSIS...\")\n",
    "\n",
    "print(f\"\\nVolume Statistics:\")\n",
    "print(f\"  Mean: {df['Volume'].mean():.2f}\")\n",
    "print(f\"  Std Dev: {df['Volume'].std():.2f}\")\n",
    "print(f\"  Min: {df['Volume'].min():.0f}\")\n",
    "print(f\"  Max: {df['Volume'].max():.0f}\")\n",
    "print(f\"  Median: {df['Volume'].median():.2f}\")\n",
    "\n",
    "# Check how many unique volume values we have\n",
    "unique_volumes = df['Volume'].nunique()\n",
    "print(f\"\\nUnique volume values: {unique_volumes}\")\n",
    "\n",
    "if unique_volumes < 100:\n",
    "    print(\"⚠ Very limited unique volume values - likely tick volume\")\n",
    "    print(\"  Tick volume (count of trades) may have limited predictive power\")\n",
    "else:\n",
    "    print(\"✓ Good variety of volume values\")\n",
    "\n",
    "# Calculate correlation between volume and price changes\n",
    "# If volume doesn't correlate with price changes, it's not useful\n",
    "price_changes = df['Close'].diff().abs()\n",
    "volume_close_corr = df['Volume'].corr(price_changes)\n",
    "\n",
    "print(f\"\\nVolume correlation with price changes: {volume_close_corr:.4f}\")\n",
    "if abs(volume_close_corr) < 0.1:\n",
    "    print(\"  → Very weak correlation (volume not predictive of price moves)\")\n",
    "else:\n",
    "    print(\"  → Moderate correlation (volume may be useful)\")\n",
    "\n",
    "# ================================================================================\n",
    "# SECTION 11: CANDLE COLOR ANALYSIS\n",
    "# ================================================================================\n",
    "# Candle color shows direction: Green (Up) or Red (Down)\n",
    "# We analyze the distribution of up/down days\n",
    "# This is relevant to your 3-candle reversal strategy\n",
    "\n",
    "print(\"\\n[SECTION 11] CANDLE COLOR ANALYSIS (UP vs DOWN CANDLES)...\")\n",
    "\n",
    "# Green candle: Close > Open (bullish)\n",
    "green_candles = (df['Close'] > df['Open']).sum()\n",
    "\n",
    "# Red candle: Close < Open (bearish)\n",
    "red_candles = (df['Close'] < df['Open']).sum()\n",
    "\n",
    "# Unchanged: Close == Open (doji)\n",
    "unchanged_candles = (df['Close'] == df['Open']).sum()\n",
    "\n",
    "total_candles = len(df)\n",
    "\n",
    "print(f\"\\nCandle Distribution:\")\n",
    "print(f\"  Green (Close > Open): {green_candles:8,} ({green_candles/total_candles*100:.2f}%)\")\n",
    "print(f\"  Red   (Close < Open): {red_candles:8,} ({red_candles/total_candles*100:.2f}%)\")\n",
    "print(f\"  Doji  (Close = Open): {unchanged_candles:8,} ({unchanged_candles/total_candles*100:.2f}%)\")\n",
    "print(f\"  Total: {total_candles:8,}\")\n",
    "\n",
    "# For your strategy, we need to understand how often we get 3 consecutive candles\n",
    "# of the same color. We'll count this in Section 15 (baseline strategy)\n",
    "\n",
    "print(f\"\\nUp/Down Ratio: {green_candles/red_candles:.4f}\")\n",
    "if green_candles > red_candles:\n",
    "    print(\"  → More up candles than down (upward bias over 20 years)\")\n",
    "else:\n",
    "    print(\"  → More down candles than up (downward bias)\")\n",
    "\n",
    "# ================================================================================\n",
    "# SECTION 12: CRISIS PERIODS ANALYSIS\n",
    "# ================================================================================\n",
    "# Historical crises affected gold prices\n",
    "# We'll analyze how XAUUSD behaved during known crisis periods\n",
    "\n",
    "print(\"\\n[SECTION 12] CRISIS PERIODS ANALYSIS...\")\n",
    "\n",
    "# Define major crisis periods\n",
    "crises = {\n",
    "    '2008 Financial Crisis': (datetime(2008, 9, 1), datetime(2008, 12, 31)),\n",
    "    '2020 COVID-19 Crash': (datetime(2020, 2, 15), datetime(2020, 4, 30)),\n",
    "    '2022 Fed Rate Hikes': (datetime(2022, 3, 1), datetime(2022, 12, 31)),\n",
    "}\n",
    "\n",
    "print(\"\\nAnalyzing price behavior during crisis periods...\\n\")\n",
    "\n",
    "for crisis_name, (start_date, end_date) in crises.items():\n",
    "    # Select data for this crisis period\n",
    "    crisis_data = df[(df.index >= start_date) & (df.index <= end_date)]\n",
    "    \n",
    "    if len(crisis_data) == 0:\n",
    "        print(f\"⚠ {crisis_name}: No data in range\")\n",
    "        continue\n",
    "    \n",
    "    # Calculate statistics for this period\n",
    "    start_price = crisis_data['Close'].iloc[0]\n",
    "    end_price = crisis_data['Close'].iloc[-1]\n",
    "    min_price = crisis_data['Close'].min()\n",
    "    max_price = crisis_data['Close'].max()\n",
    "    avg_price = crisis_data['Close'].mean()\n",
    "    \n",
    "    # Calculate returns during crisis\n",
    "    period_return = (end_price - start_price) / start_price * 100\n",
    "    drawdown = (min_price - crisis_data['Close'].iloc[0]) / crisis_data['Close'].iloc[0] * 100\n",
    "    \n",
    "    # Volatility during crisis\n",
    "    crisis_volatility = crisis_data['Log_Return'].std()\n",
    "    \n",
    "    print(f\"{crisis_name}:\")\n",
    "    print(f\"  Period: {start_date.date()} to {end_date.date()}\")\n",
    "    print(f\"  Start Price: ${start_price:.2f}\")\n",
    "    print(f\"  End Price: ${end_price:.2f}\")\n",
    "    print(f\"  Period Return: {period_return:+.2f}%\")\n",
    "    print(f\"  Max Drawdown: {drawdown:.2f}%\")\n",
    "    print(f\"  Price Range: ${min_price:.2f} - ${max_price:.2f}\")\n",
    "    print(f\"  Volatility: {crisis_volatility:.6f} (daily returns std)\")\n",
    "    print()\n",
    "\n",
    "# ================================================================================\n",
    "# SECTION 13: MAXIMUM DRAWDOWN ANALYSIS\n",
    "# ================================================================================\n",
    "# Drawdown is the peak-to-trough decline\n",
    "# Maximum drawdown shows the worst loss experienced\n",
    "# Important for risk management\n",
    "\n",
    "print(\"\\n[SECTION 13] MAXIMUM DRAWDOWN ANALYSIS...\")\n",
    "\n",
    "# Calculate cumulative maximum price at each point\n",
    "cumulative_max = df['Close'].cummax()\n",
    "\n",
    "# Calculate drawdown at each point\n",
    "# Drawdown = (Current Price - Peak) / Peak\n",
    "drawdown = (df['Close'] - cumulative_max) / cumulative_max * 100\n",
    "\n",
    "# Find maximum drawdown\n",
    "max_drawdown = drawdown.min()\n",
    "\n",
    "# Find when the maximum drawdown occurred\n",
    "max_drawdown_date = drawdown.idxmin()\n",
    "\n",
    "print(f\"\\nMaximum Drawdown: {max_drawdown:.2f}%\")\n",
    "print(f\"Occurred on: {max_drawdown_date}\")\n",
    "print(f\"Price at that time: ${df.loc[max_drawdown_date, 'Close']:.2f}\")\n",
    "\n",
    "# Calculate average and rolling maximum drawdowns\n",
    "avg_drawdown = drawdown[drawdown < 0].mean()\n",
    "print(f\"\\nAverage Drawdown (when negative): {avg_drawdown:.2f}%\")\n",
    "\n",
    "# Find longest drawdown period\n",
    "# A drawdown period is when price is below recent peak\n",
    "drawdown_periods = (drawdown < 0).astype(int)\n",
    "drawdown_changes = drawdown_periods.diff().fillna(0)\n",
    "drawdown_starts = drawdown_changes[drawdown_changes == 1].index\n",
    "drawdown_ends = drawdown_changes[drawdown_changes == -1].index\n",
    "\n",
    "if len(drawdown_starts) > 0 and len(drawdown_ends) > 0:\n",
    "    longest_duration = 0\n",
    "    longest_start = None\n",
    "    \n",
    "    for start in drawdown_starts:\n",
    "        matching_end = drawdown_ends[drawdown_ends > start]\n",
    "        if len(matching_end) > 0:\n",
    "            end = matching_end[0]\n",
    "            duration = (end - start).total_seconds() / 3600  # Convert to hours\n",
    "            if duration > longest_duration:\n",
    "                longest_duration = duration\n",
    "                longest_start = start\n",
    "    \n",
    "    print(f\"Longest drawdown period: {longest_duration:.0f} hours\")\n",
    "\n",
    "# ================================================================================\n",
    "# SECTION 14: SPREAD AND SLIPPAGE ESTIMATION\n",
    "# ================================================================================\n",
    "# Spreads and slippage are transaction costs that reduce profitability\n",
    "# We estimate realistic spreads based on the data\n",
    "\n",
    "print(\"\\n[SECTION 14] SPREAD AND SLIPPAGE ANALYSIS...\")\n",
    "\n",
    "# Bid-Ask Spread estimation:\n",
    "# We can estimate spread by looking at the High-Low range within a candle\n",
    "# Typical XAUUSD spread from brokers is 2-3 pips\n",
    "\n",
    "# Calculate average range per candle\n",
    "df['Candle_Range'] = df['High'] - df['Low']\n",
    "avg_range = df['Candle_Range'].mean()\n",
    "\n",
    "print(f\"\\nAverage candle range: ${avg_range:.4f}\")\n",
    "print(f\"  (This is not spread, but shows typical intracandle movement)\")\n",
    "\n",
    "# For XAUUSD, typical brokers have these spreads:\n",
    "typical_spread = 0.30  # 30 cents on gold is ~3 pips (10 cents per pip)\n",
    "print(f\"\\nTypical XAUUSD spreads from brokers:\")\n",
    "print(f\"  Standard brokers: 2-4 pips (~$0.20-$0.40)\")\n",
    "print(f\"  ECN brokers: 0.5-1.5 pips (~$0.05-$0.15)\")\n",
    "\n",
    "# Slippage is additional cost when executing large orders\n",
    "# We'll assume 1-2 pips for this analysis\n",
    "typical_slippage = 0.15  # 1.5 pips\n",
    "total_cost_per_trade = (typical_spread + typical_slippage)\n",
    "\n",
    "print(f\"  Total cost per round-trip trade (entry + exit): ~${total_cost_per_trade:.2f}\")\n",
    "print(f\"\\n⚠ Important: These costs must be included in backtesting!\")\n",
    "print(f\"  A strategy needs to make at least this much to be profitable\")\n",
    "\n",
    "# ================================================================================\n",
    "# SECTION 15: BASELINE STRATEGY ANALYSIS - YOUR 3-CANDLE REVERSAL\n",
    "# ================================================================================\n",
    "# YOUR EXACT STRATEGY (Final Version):\n",
    "# BUY: 3 consecutive GREEN candles → Enter at close of 3rd → SL at midpoint of 2nd\n",
    "# SELL: 3 consecutive RED candles → Enter at close of 3rd → SL at midpoint of 2nd\n",
    "# TP: Always 2× the SL distance\n",
    "# EXIT: Only at SL or TP (candle wicks ignored)\n",
    "# REQUIREMENT: Exactly 3 consecutive same-color candles (no partial patterns)\n",
    "\n",
    "print(\"\\n[SECTION 15] BASELINE STRATEGY ANALYSIS - YOUR 3-CANDLE PATTERN...\")\n",
    "print(\"\\nStrategy Rules (FINAL):\")\n",
    "print(\"  BUY:  3 GREEN candles → Enter @ close of 3rd → SL @ midpoint of 2nd\")\n",
    "print(\"  SELL: 3 RED candles → Enter @ close of 3rd → SL @ midpoint of 2nd\")\n",
    "print(\"  TP: Entry ± 2×(SL distance)\")\n",
    "print(\"  EXIT: Only at SL or TP\")\n",
    "print(\"  REQUIREMENT: Exactly 3 consecutive same-color candles\")\n",
    "\n",
    "# Create candle color column\n",
    "# GREEN = 1 (Close > Open), RED = 0 (Close < Open)\n",
    "df['Candle_Color'] = (df['Close'] > df['Open']).astype(int)\n",
    "\n",
    "signals = []  # Store all entry signals found\n",
    "\n",
    "# Loop through data looking for exactly 3 consecutive same-color candles\n",
    "# We start at index 2 because we need candles at i-2, i-1, and i\n",
    "# We stop before the end to allow for entry at i+1\n",
    "for i in range(2, len(df) - 1):\n",
    "    # Get colors of the last 3 candles (i-2, i-1, i)\n",
    "    color_at_i_minus_2 = df['Candle_Color'].iloc[i-2]\n",
    "    color_at_i_minus_1 = df['Candle_Color'].iloc[i-1]\n",
    "    color_at_i = df['Candle_Color'].iloc[i]\n",
    "    \n",
    "    # Check if all 3 candles are exactly the same color\n",
    "    # This means either all are GREEN (1,1,1) or all are RED (0,0,0)\n",
    "    if (color_at_i_minus_2 == color_at_i_minus_1 == color_at_i):\n",
    "        \n",
    "        # We found 3 consecutive same-color candles!\n",
    "        # According to your strategy:\n",
    "        # - Candle at i-2 = 1st candle of pattern\n",
    "        # - Candle at i-1 = 2nd candle of pattern (SL will be calculated from this)\n",
    "        # - Candle at i = 3rd candle of pattern\n",
    "        # - Entry = close of candle at i+1 (next candle after the pattern)\n",
    "        \n",
    "        if (i + 1) < len(df):  # Make sure we have a next candle for entry\n",
    "            \n",
    "            # Entry details\n",
    "            entry_index = i + 1\n",
    "            entry_date = df.index[entry_index]\n",
    "            entry_price = df['Close'].iloc[entry_index]  # Entry at close of 3rd candle\n",
    "            \n",
    "            # Determine signal type based on color\n",
    "            if color_at_i == 1:  # All three are GREEN\n",
    "                signal_type = 'BUY'\n",
    "                # SL = midpoint of 2nd green candle (at i-1)\n",
    "                sl_high = df['High'].iloc[i-1]\n",
    "                sl_low = df['Low'].iloc[i-1]\n",
    "                sl_price = (sl_high + sl_low) / 2\n",
    "                \n",
    "            else:  # All three are RED (color_at_i == 0)\n",
    "                signal_type = 'SELL'\n",
    "                # SL = midpoint of 2nd red candle (at i-1)\n",
    "                sl_high = df['High'].iloc[i-1]\n",
    "                sl_low = df['Low'].iloc[i-1]\n",
    "                sl_price = (sl_high + sl_low) / 2\n",
    "            \n",
    "            # Calculate Take Profit based on SL distance\n",
    "            if signal_type == 'BUY':\n",
    "                # For BUY: Entry is below TP, SL is below Entry\n",
    "                sl_distance = entry_price - sl_price  # How far SL is below entry\n",
    "                tp_distance = 2 * sl_distance  # TP is 2x the risk\n",
    "                tp_price = entry_price + tp_distance  # TP = Entry + 2×risk\n",
    "                \n",
    "            else:  # SELL\n",
    "                # For SELL: Entry is above TP, SL is above Entry\n",
    "                sl_distance = sl_price - entry_price  # How far SL is above entry\n",
    "                tp_distance = 2 * sl_distance  # TP is 2x the risk\n",
    "                tp_price = entry_price - tp_distance  # TP = Entry - 2×risk\n",
    "            \n",
    "            # Store this signal\n",
    "            signals.append({\n",
    "                'Entry_Date': entry_date,\n",
    "                'Entry_Index': entry_index,\n",
    "                'Pattern_Candle_1_Index': i-2,\n",
    "                'Pattern_Candle_2_Index': i-1,\n",
    "                'Pattern_Candle_3_Index': i,\n",
    "                'Signal_Type': signal_type,\n",
    "                'Entry_Price': entry_price,\n",
    "                'SL_Price': sl_price,\n",
    "                'SL_Distance': abs(sl_distance),\n",
    "                'TP_Price': tp_price,\n",
    "                'TP_Distance': tp_distance\n",
    "            })\n",
    "\n",
    "print(f\"\\n✓ Pattern Recognition Complete\")\n",
    "print(f\"  Total patterns found: {len(signals)}\")\n",
    "print(f\"  Data range: {df.index[0]} to {df.index[-1]}\")\n",
    "\n",
    "# Now simulate each signal as a trade\n",
    "# For each signal, we look forward to see if SL or TP is hit first\n",
    "trades = []\n",
    "\n",
    "for signal in signals:\n",
    "    entry_index = signal['Entry_Index']\n",
    "    signal_type = signal['Signal_Type']\n",
    "    entry_price = signal['Entry_Price']\n",
    "    sl_price = signal['SL_Price']\n",
    "    tp_price = signal['TP_Price']\n",
    "    entry_date = signal['Entry_Date']\n",
    "    \n",
    "    # Trade state\n",
    "    exit_price = None\n",
    "    exit_reason = None\n",
    "    bars_held = 0\n",
    "    \n",
    "    # Scan forward from entry to find where SL or TP is hit\n",
    "    # We limit the search to 1440 candles (24 hours of 1-minute data)\n",
    "    max_bars_to_check = 1440\n",
    "    \n",
    "    for j in range(entry_index + 1, min(entry_index + 1 + max_bars_to_check, len(df))):\n",
    "        current_high = df['High'].iloc[j]\n",
    "        current_low = df['Low'].iloc[j]\n",
    "        bars_held = j - entry_index\n",
    "        \n",
    "        if signal_type == 'BUY':\n",
    "            # For BUY: We're long, looking for price to go UP to TP or DOWN to SL\n",
    "            # SL is below entry, TP is above entry\n",
    "            \n",
    "            # Check if SL is hit (price drops to or below SL)\n",
    "            if current_low <= sl_price:\n",
    "                # SL hit - we lose money\n",
    "                exit_price = sl_price\n",
    "                exit_reason = 'SL'\n",
    "                break\n",
    "            \n",
    "            # Check if TP is hit (price rises to or above TP)\n",
    "            elif current_high >= tp_price:\n",
    "                # TP hit - we make money\n",
    "                exit_price = tp_price\n",
    "                exit_reason = 'TP'\n",
    "                break\n",
    "        \n",
    "        else:  # SELL\n",
    "            # For SELL: We're short, looking for price to go DOWN to TP or UP to SL\n",
    "            # SL is above entry, TP is below entry\n",
    "            \n",
    "            # Check if SL is hit (price rises to or above SL)\n",
    "            if current_high >= sl_price:\n",
    "                # SL hit - we lose money\n",
    "                exit_price = sl_price\n",
    "                exit_reason = 'SL'\n",
    "                break\n",
    "            \n",
    "            # Check if TP is hit (price falls to or below TP)\n",
    "            elif current_low <= tp_price:\n",
    "                # TP hit - we make money\n",
    "                exit_price = tp_price\n",
    "                exit_reason = 'TP'\n",
    "                break\n",
    "    \n",
    "    # If we didn't hit SL or TP within 24 hours, trade is still open\n",
    "    # We'll close it at the last available price\n",
    "    if exit_reason is None:\n",
    "        exit_reason = 'TIMEOUT'\n",
    "        exit_price = df['Close'].iloc[-1]\n",
    "        # Don't count timeouts as real trades (they're incomplete)\n",
    "    \n",
    "    # Calculate profit/loss\n",
    "    if signal_type == 'BUY':\n",
    "        pnl = exit_price - entry_price\n",
    "        pnl_pct = (pnl / entry_price) * 100\n",
    "    else:  # SELL\n",
    "        pnl = entry_price - exit_price\n",
    "        pnl_pct = (pnl / entry_price) * 100\n",
    "    \n",
    "    # Record the trade\n",
    "    trades.append({\n",
    "        'Entry_Date': entry_date,\n",
    "        'Signal_Type': signal_type,\n",
    "        'Entry_Price': entry_price,\n",
    "        'SL_Price': sl_price,\n",
    "        'TP_Price': tp_price,\n",
    "        'Exit_Price': exit_price,\n",
    "        'Exit_Reason': exit_reason,\n",
    "        'PnL': pnl,\n",
    "        'PnL_Pct': pnl_pct,\n",
    "        'Bars_Held': bars_held,\n",
    "        'Is_Win': pnl > 0,\n",
    "        'Is_Completed': exit_reason in ['SL', 'TP']\n",
    "    })\n",
    "\n",
    "# Filter to only completed trades (those that hit SL or TP)\n",
    "completed_trades = [t for t in trades if t['Is_Completed']]\n",
    "trades_df = pd.DataFrame(completed_trades)\n",
    "\n",
    "print(f\"\\n✓ Trade Simulation Complete\")\n",
    "print(f\"  Total signals: {len(signals)}\")\n",
    "print(f\"  Total trades executed: {len(trades)}\")\n",
    "print(f\"  Completed trades (hit SL/TP): {len(trades_df)}\")\n",
    "print(f\"  Incomplete trades (timeout): {len(trades) - len(trades_df)}\")\n",
    "\n",
    "# Calculate statistics from completed trades only\n",
    "if len(trades_df) > 0:\n",
    "    winning_trades = trades_df['Is_Win'].sum()\n",
    "    losing_trades = (~trades_df['Is_Win']).sum()\n",
    "    win_rate = (winning_trades / len(trades_df)) * 100\n",
    "    \n",
    "    avg_win = trades_df[trades_df['Is_Win']]['PnL'].mean()\n",
    "    avg_loss = trades_df[~trades_df['Is_Win']]['PnL'].mean()\n",
    "    \n",
    "    total_pnl = trades_df['PnL'].sum()\n",
    "    gross_profit = trades_df[trades_df['Is_Win']]['PnL'].sum()\n",
    "    gross_loss = abs(trades_df[~trades_df['Is_Win']]['PnL'].sum())\n",
    "    \n",
    "    profit_factor = gross_profit / gross_loss if gross_loss > 0 else 0\n",
    "    \n",
    "    avg_bars_held = trades_df['Bars_Held'].mean()\n",
    "    \n",
    "    # Sharpe ratio (simplified): return / volatility\n",
    "    returns = trades_df['PnL_Pct']\n",
    "    sharpe = (returns.mean() / returns.std()) * np.sqrt(252) if returns.std() > 0 else 0\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"BASELINE STRATEGY RESULTS (YOUR 3-CANDLE PATTERN)\")\n",
    "    print(f\"=\"*80)\n",
    "    print(f\"\\nTrade Count:\")\n",
    "    print(f\"  Total Completed Trades: {len(trades_df)}\")\n",
    "    print(f\"  Winning Trades: {winning_trades} ({win_rate:.2f}%)\")\n",
    "    print(f\"  Losing Trades: {losing_trades} ({100-win_rate:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nProfitability:\")\n",
    "    print(f\"  Total P&L: ${total_pnl:.4f}\")\n",
    "    print(f\"  Gross Profit: ${gross_profit:.4f}\")\n",
    "    print(f\"  Gross Loss: ${gross_loss:.4f}\")\n",
    "    print(f\"  Profit Factor: {profit_factor:.2f} (>1.2 is good)\")\n",
    "    \n",
    "    print(f\"\\nAverage Trade:\")\n",
    "    print(f\"  Avg Win: ${avg_win:.4f}\")\n",
    "    print(f\"  Avg Loss: ${avg_loss:.4f}\")\n",
    "    print(f\"  Win/Loss Ratio: {avg_win/abs(avg_loss) if avg_loss != 0 else 0:.2f}\")\n",
    "    print(f\"  Avg Bars Held: {avg_bars_held:.0f} minutes\")\n",
    "    \n",
    "    print(f\"\\nRisk-Adjusted:\")\n",
    "    print(f\"  Sharpe Ratio: {sharpe:.4f}\")\n",
    "    print(f\"  Return per Trade (avg): {returns.mean():.6f}%\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"STRATEGY EDGE ASSESSMENT:\")\n",
    "    print(f\"=\"*80)\n",
    "    \n",
    "    # Assessment based on metrics\n",
    "    if win_rate >= 40:\n",
    "        print(f\"✓ Win rate {win_rate:.2f}% >= 40% threshold (profitable with 1:2 risk-reward)\")\n",
    "    else:\n",
    "        print(f\"✗ Win rate {win_rate:.2f}% < 40% threshold (marginal edge)\")\n",
    "    \n",
    "    if profit_factor >= 1.2:\n",
    "        print(f\"✓ Profit factor {profit_factor:.2f} >= 1.2 (good)\")\n",
    "    else:\n",
    "        print(f\"✗ Profit factor {profit_factor:.2f} < 1.2 (weak)\")\n",
    "    \n",
    "    if sharpe >= 0.7:\n",
    "        print(f\"✓ Sharpe ratio {sharpe:.4f} >= 0.7 (acceptable)\")\n",
    "    else:\n",
    "        print(f\"✗ Sharpe ratio {sharpe:.4f} < 0.7 (poor risk-adjusted returns)\")\n",
    "    \n",
    "    # Overall assessment\n",
    "    edge_score = 0\n",
    "    if win_rate >= 40: edge_score += 1\n",
    "    if profit_factor >= 1.2: edge_score += 1\n",
    "    if sharpe >= 0.7: edge_score += 1\n",
    "    \n",
    "    print(f\"\\nEdge Score: {edge_score}/3\")\n",
    "    if edge_score >= 2:\n",
    "        print(f\"→ EDGE EXISTS: Strategy shows statistically significant advantage\")\n",
    "        print(f\"→ ML MODEL should aim to beat this baseline\")\n",
    "    else:\n",
    "        print(f\"→ MARGINAL EDGE: Strategy is borderline, needs careful tuning\")\n",
    "\n",
    "# ================================================================================\n",
    "# SECTION 16: VISUALIZATIONS\n",
    "# ================================================================================\n",
    "# Now we create visualizations to see patterns visually\n",
    "\n",
    "print(\"\\n[SECTION 16] CREATING VISUALIZATIONS...\")\n",
    "\n",
    "# Visualization 1: Full price history with crisis periods marked\n",
    "fig, ax = plt.subplots(figsize=(16, 7))\n",
    "\n",
    "ax.plot(df.index, df['Close'], linewidth=0.5, color='steelblue', alpha=0.7)\n",
    "ax.fill_between(df.index, df['Low'], df['High'], alpha=0.15, color='steelblue')\n",
    "\n",
    "# Mark crisis periods\n",
    "colors_crisis = {'2008 Financial Crisis': 'red', '2020 COVID-19 Crash': 'orange', '2022 Fed Rate Hikes': 'purple'}\n",
    "for crisis_name, (start_date, end_date) in crises.items():\n",
    "    ax.axvspan(start_date, end_date, alpha=0.1, color=colors_crisis[crisis_name], label=crisis_name)\n",
    "\n",
    "ax.set_title('XAUUSD Price History (2004-2024) with Crisis Periods', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Price (USD)', fontsize=12)\n",
    "ax.legend(loc='upper left', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('01_price_history_with_crises.png', dpi=300, bbox_inches='tight')\n",
    "print(\"  ✓ Saved: 01_price_history_with_crises.png\")\n",
    "plt.close()\n",
    "\n",
    "# Visualization 2: Returns distribution (histogram)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Simple returns\n",
    "axes[0].hist(df['Simple_Return'].dropna() * 100, bins=100, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(df['Simple_Return'].mean() * 100, color='red', linestyle='--', linewidth=2, label=f'Mean: {df[\"Simple_Return\"].mean()*100:.4f}%')\n",
    "axes[0].set_title('Simple Returns Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Return (%)', fontsize=11)\n",
    "axes[0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Log returns\n",
    "axes[1].hist(df['Log_Return'].dropna() * 100, bins=100, color='darkgreen', alpha=0.7, edgecolor='black')\n",
    "axes[1].axvline(df['Log_Return'].mean() * 100, color='red', linestyle='--', linewidth=2, label=f'Mean: {df[\"Log_Return\"].mean()*100:.4f}%')\n",
    "axes[1].set_title('Log Returns Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Return (%)', fontsize=11)\n",
    "axes[1].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('02_returns_distribution.png', dpi=300, bbox_inches='tight')\n",
    "print(\"  ✓ Saved: 02_returns_distribution.png\")\n",
    "plt.close()\n",
    "\n",
    "# Visualization 3: Volatility over time\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "ax.plot(df.index, df['Volatility_1d'], linewidth=1, color='purple', alpha=0.7, label='1-Day Rolling Volatility')\n",
    "ax.fill_between(df.index, df['Volatility_1d'], alpha=0.2, color='purple')\n",
    "\n",
    "# Mark crisis periods\n",
    "for crisis_name, (start_date, end_date) in crises.items():\n",
    "    ax.axvspan(start_date, end_date, alpha=0.1, color=colors_crisis[crisis_name])\n",
    "\n",
    "ax.set_title('XAUUSD Rolling Volatility (1-Day Window)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Daily Volatility (Std Dev)', fontsize=12)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('03_volatility_over_time.png', dpi=300, bbox_inches='tight')\n",
    "print(\"  ✓ Saved: 03_volatility_over_time.png\")\n",
    "plt.close()\n",
    "\n",
    "# Visualization 4: Drawdown over time\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "cumulative_max = df['Close'].cummax()\n",
    "drawdown = ((df['Close'] - cumulative_max) / cumulative_max) * 100\n",
    "\n",
    "ax.fill_between(df.index, drawdown, 0, color='red', alpha=0.5, label='Drawdown')\n",
    "ax.plot(df.index, drawdown, color='darkred', linewidth=0.8)\n",
    "\n",
    "ax.set_title('XAUUSD Drawdown Over Time', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Drawdown (%)', fontsize=12)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('04_drawdown_over_time.png', dpi=300, bbox_inches='tight')\n",
    "print(\"  ✓ Saved: 04_drawdown_over_time.png\")\n",
    "plt.close()\n",
    "\n",
    "# Visualization 5: Candle colors (green vs red) over time\n",
    "fig, ax = plt.subplots(figsize=(16, 5))\n",
    "\n",
    "# Create a color array: green for up days, red for down days\n",
    "colors = ['green' if color == 1 else 'red' for color in df['Candle_Color']]\n",
    "\n",
    "# Show last 5000 candles for visibility\n",
    "last_n = 5000\n",
    "x_vals = range(len(df) - last_n, len(df))\n",
    "y_vals = df['Close'].iloc[-last_n:].values\n",
    "\n",
    "for i, (x, y, color) in enumerate(zip(x_vals, y_vals, colors[-last_n:])):\n",
    "    ax.scatter(x, y, c=color, s=1, alpha=0.6)\n",
    "\n",
    "ax.set_title(f'XAUUSD Candle Colors (Last {last_n} Candles)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Candle Index', fontsize=12)\n",
    "ax.set_ylabel('Close Price', fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('05_candle_colors_last_5000.png', dpi=300, bbox_inches='tight')\n",
    "print(\"  ✓ Saved: 05_candle_colors_last_5000.png\")\n",
    "plt.close()\n",
    "\n",
    "# ================================================================================\n",
    "# SECTION 17: BIVARIATE ANALYSIS - OHLC RELATIONSHIPS\n",
    "# ================================================================================\n",
    "# How do Open, High, Low, Close relate to each other?\n",
    "\n",
    "print(\"\\n[SECTION 17] BIVARIATE ANALYSIS - OHLC RELATIONSHIPS...\")\n",
    "\n",
    "# Check: Does Close tend to be near High or Low?\n",
    "df['Close_Position'] = (df['Close'] - df['Low']) / (df['High'] - df['Low'])\n",
    "\n",
    "print(f\"\\nClose Position within Daily Range:\")\n",
    "print(f\"  Mean: {df['Close_Position'].mean():.4f}\")\n",
    "print(f\"  (0 = Close at Low, 1 = Close at High, 0.5 = Middle)\")\n",
    "\n",
    "if df['Close_Position'].mean() > 0.5:\n",
    "    print(f\"  → Close typically above middle (bullish tendency)\")\n",
    "else:\n",
    "    print(f\"  → Close typically below middle (bearish tendency)\")\n",
    "\n",
    "# Correlation between OHLC\n",
    "print(f\"\\nOHLC Correlation Matrix:\")\n",
    "ohlc_corr = df[['Open', 'High', 'Low', 'Close']].corr()\n",
    "print(ohlc_corr)\n",
    "\n",
    "# ================================================================================\n",
    "# SECTION 18: SUMMARY REPORT\n",
    "# ================================================================================\n",
    "# Final summary of key findings\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY REPORT - GENERAL EDA FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_findings = {\n",
    "    'Data Period': f\"{df.index[0].date()} to {df.index[-1].date()}\",\n",
    "    'Total Candles': f\"{len(df):,}\",\n",
    "    'Timezone': detected_timezone,\n",
    "    'Data Quality': 'Excellent',\n",
    "    'Price Range': f\"${df['Close'].min():.2f} - ${df['Close'].max():.2f}\",\n",
    "    'Average Daily Volatility': f\"{df['Volatility_1d'].mean():.6f}\",\n",
    "    'Annualized Volatility': f\"{annual_volatility*100:.2f}%\",\n",
    "    'Max Drawdown': f\"{max_drawdown:.2f}%\",\n",
    "    'Green vs Red Candles': f\"{(green_candles/total_candles*100):.2f}% up, {(red_candles/total_candles*100):.2f}% down\",\n",
    "    'Baseline Strategy Signals': f\"{len(signals)}\",\n",
    "    'Baseline Win Rate': f\"{win_rate:.2f}%\" if len(trades) > 0 else \"N/A\",\n",
    "    'Volume Type': 'Likely Tick Volume (limited predictive power)',\n",
    "    'Key Finding': 'Data is clean, 2008/2020 crises visible, volatility regimes detectable',\n",
    "}\n",
    "\n",
    "for key, value in summary_findings.items():\n",
    "    print(f\"{key:.<50} {value}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n✓ GENERAL EDA COMPLETE!\")\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"  1. Review all visualizations (PNG files)\")\n",
    "print(\"  2. Document findings in EXPLORATION_LOG.md\")\n",
    "print(\"  3. Use insights for feature engineering\")\n",
    "print(\"  4. Build baseline model based on 3-candle strategy\")\n",
    "print(\"  5. Move to formal trading-system architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49d48b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# SECTION 15: BASELINE STRATEGY ANALYSIS - YOUR 3-CANDLE REVERSAL\n",
    "# ================================================================================\n",
    "# YOUR EXACT STRATEGY (Final Version):\n",
    "# BUY: 3 consecutive GREEN candles → Enter at close of 3rd → SL at midpoint of 2nd\n",
    "# SELL: 3 consecutive RED candles → Enter at close of 3rd → SL at midpoint of 2nd\n",
    "# TP: Always 2× the SL distance\n",
    "# EXIT: Only at SL or TP (candle wicks ignored)\n",
    "# REQUIREMENT: Exactly 3 consecutive same-color candles (no partial patterns)\n",
    "\n",
    "print(\"\\n[SECTION 15] BASELINE STRATEGY ANALYSIS - YOUR 3-CANDLE PATTERN...\")\n",
    "print(\"\\nStrategy Rules (FINAL):\")\n",
    "print(\"  BUY:  3 GREEN candles → Enter @ close of 3rd → SL @ midpoint of 2nd\")\n",
    "print(\"  SELL: 3 RED candles → Enter @ close of 3rd → SL @ midpoint of 2nd\")\n",
    "print(\"  TP: Entry ± 2×(SL distance)\")\n",
    "print(\"  EXIT: Only at SL or TP\")\n",
    "print(\"  REQUIREMENT: Exactly 3 consecutive same-color candles\")\n",
    "\n",
    "# ================================================================================\n",
    "# TRANSACTION COST CONFIGURATION\n",
    "# ================================================================================\n",
    "# Define transaction costs for different broker types\n",
    "BROKER_COSTS = {\n",
    "    'standard': 0.45,  # Standard broker: 3 pips spread + slippage\n",
    "    'ecn': 0.20        # ECN broker (e.g., Exness Raw Spread): tight spread + commission\n",
    "}\n",
    "\n",
    "# Select broker type for analysis\n",
    "# Change this to 'ecn' if you want to test with ECN broker costs\n",
    "SELECTED_BROKER = 'standard'  # Options: 'standard' or 'ecn'\n",
    "transaction_cost_per_trade = BROKER_COSTS[SELECTED_BROKER]\n",
    "\n",
    "print(f\"\\n⚠ TRANSACTION COST CONFIGURATION:\")\n",
    "print(f\"  Broker Type: {SELECTED_BROKER.upper()}\")\n",
    "print(f\"  Cost per Round-Trip Trade: ${transaction_cost_per_trade:.2f}\")\n",
    "print(f\"  (Change SELECTED_BROKER variable to 'ecn' for lower costs)\")\n",
    "\n",
    "# Create candle color column\n",
    "# GREEN = 1 (Close > Open), RED = 0 (Close < Open)\n",
    "df['Candle_Color'] = (df['Close'] > df['Open']).astype(int)\n",
    "\n",
    "signals = []  # Store all entry signals found\n",
    "\n",
    "# Loop through data looking for exactly 3 consecutive same-color candles\n",
    "# We start at index 2 because we need candles at i-2, i-1, and i\n",
    "# We stop before the end to allow for entry at i+1\n",
    "for i in range(2, len(df) - 1):\n",
    "    # Get colors of the last 3 candles (i-2, i-1, i)\n",
    "    color_at_i_minus_2 = df['Candle_Color'].iloc[i-2]\n",
    "    color_at_i_minus_1 = df['Candle_Color'].iloc[i-1]\n",
    "    color_at_i = df['Candle_Color'].iloc[i]\n",
    "    \n",
    "    # Check if all 3 candles are exactly the same color\n",
    "    # This means either all are GREEN (1,1,1) or all are RED (0,0,0)\n",
    "    if (color_at_i_minus_2 == color_at_i_minus_1 == color_at_i):\n",
    "        \n",
    "        # We found 3 consecutive same-color candles!\n",
    "        # According to your strategy:\n",
    "        # - Candle at i-2 = 1st candle of pattern\n",
    "        # - Candle at i-1 = 2nd candle of pattern (SL will be calculated from this)\n",
    "        # - Candle at i = 3rd candle of pattern\n",
    "        # - Entry = immediately at the CLOSE of the 3rd candle (i)\n",
    "            \n",
    "        # Entry details\n",
    "        entry_index = i\n",
    "        entry_date = df.index[entry_index]\n",
    "        entry_price = df['Close'].iloc[entry_index]  # Entry at close of 3rd candle\n",
    "        \n",
    "        # Determine signal type based on color\n",
    "        if color_at_i == 1:  # All three are GREEN\n",
    "            signal_type = 'BUY'\n",
    "            # SL = midpoint of 2nd green candle (at i-1)\n",
    "            sl_high = df['High'].iloc[i-1]\n",
    "            sl_low = df['Low'].iloc[i-1]\n",
    "            sl_price = (sl_high + sl_low) / 2\n",
    "                \n",
    "        else:  # All three are RED (color_at_i == 0)\n",
    "            signal_type = 'SELL'\n",
    "            # SL = midpoint of 2nd red candle (at i-1)\n",
    "            sl_high = df['High'].iloc[i-1]\n",
    "            sl_low = df['Low'].iloc[i-1]\n",
    "            sl_price = (sl_high + sl_low) / 2\n",
    "            \n",
    "        # Calculate Take Profit based on SL distance\n",
    "        if signal_type == 'BUY':\n",
    "            # For BUY: Entry is below TP, SL is below Entry\n",
    "            sl_distance = entry_price - sl_price  # How far SL is below entry\n",
    "            tp_distance = 2 * sl_distance  # TP is 2x the risk\n",
    "            tp_price = entry_price + tp_distance  # TP = Entry + 2×risk\n",
    "            \n",
    "        else:  # SELL\n",
    "            # For SELL: Entry is above TP, SL is above Entry\n",
    "            sl_distance = sl_price - entry_price  # How far SL is above entry\n",
    "            tp_distance = 2 * sl_distance  # TP is 2x the risk\n",
    "            tp_price = entry_price - tp_distance  # TP = Entry - 2×risk\n",
    "            \n",
    "        # Store this signal\n",
    "        signals.append({\n",
    "            'Entry_Date': entry_date,\n",
    "            'Entry_Index': entry_index,\n",
    "            'Pattern_Candle_1_Index': i-2,\n",
    "            'Pattern_Candle_2_Index': i-1,\n",
    "            'Pattern_Candle_3_Index': i,\n",
    "            'Signal_Type': signal_type,\n",
    "            'Entry_Price': entry_price,\n",
    "            'SL_Price': sl_price,\n",
    "            'SL_Distance': abs(sl_distance),\n",
    "            'TP_Price': tp_price,\n",
    "            'TP_Distance': tp_distance\n",
    "        })\n",
    "\n",
    "print(f\"\\n✓ Pattern Recognition Complete\")\n",
    "print(f\"  Total patterns found: {len(signals)}\")\n",
    "print(f\"  Data range: {df.index[0]} to {df.index[-1]}\")\n",
    "\n",
    "# Now simulate each signal as a trade\n",
    "# For each signal, we look forward to see if SL or TP is hit first\n",
    "trades = []\n",
    "\n",
    "for signal in signals:\n",
    "    entry_index = signal['Entry_Index']\n",
    "    signal_type = signal['Signal_Type']\n",
    "    entry_price = signal['Entry_Price']\n",
    "    sl_price = signal['SL_Price']\n",
    "    tp_price = signal['TP_Price']\n",
    "    entry_date = signal['Entry_Date']\n",
    "    \n",
    "    # Trade state\n",
    "    exit_price = None\n",
    "    exit_reason = None\n",
    "    bars_held = 0\n",
    "    \n",
    "    # Scan forward from entry to find where SL or TP is hit\n",
    "    # We limit the search to 1440 candles (24 hours of 1-minute data)\n",
    "    max_bars_to_check = 1440\n",
    "    \n",
    "    for j in range(entry_index + 1, min(entry_index + 1 + max_bars_to_check, len(df))):\n",
    "        current_high = df['High'].iloc[j]\n",
    "        current_low = df['Low'].iloc[j]\n",
    "        bars_held = j - entry_index\n",
    "        \n",
    "        if signal_type == 'BUY':\n",
    "            # For BUY: We're long, looking for price to go UP to TP or DOWN to SL\n",
    "            # SL is below entry, TP is above entry\n",
    "            \n",
    "            # Check if SL is hit (price drops to or below SL)\n",
    "            if current_low <= sl_price:\n",
    "                # SL hit - we lose money\n",
    "                exit_price = sl_price\n",
    "                exit_reason = 'SL'\n",
    "                break\n",
    "            \n",
    "            # Check if TP is hit (price rises to or above TP)\n",
    "            elif current_high >= tp_price:\n",
    "                # TP hit - we make money\n",
    "                exit_price = tp_price\n",
    "                exit_reason = 'TP'\n",
    "                break\n",
    "        \n",
    "        else:  # SELL\n",
    "            # For SELL: We're short, looking for price to go DOWN to TP or UP to SL\n",
    "            # SL is above entry, TP is below entry\n",
    "            \n",
    "            # Check if SL is hit (price rises to or above SL)\n",
    "            if current_high >= sl_price:\n",
    "                # SL hit - we lose money\n",
    "                exit_price = sl_price\n",
    "                exit_reason = 'SL'\n",
    "                break\n",
    "            \n",
    "            # Check if TP is hit (price falls to or below TP)\n",
    "            elif current_low <= tp_price:\n",
    "                # TP hit - we make money\n",
    "                exit_price = tp_price\n",
    "                exit_reason = 'TP'\n",
    "                break\n",
    "    \n",
    "    # If we didn't hit SL or TP within 24 hours, trade is still open\n",
    "    # We'll close it at the last available price\n",
    "    if exit_reason is None:\n",
    "        exit_reason = 'TIMEOUT'\n",
    "        exit_price = df['Close'].iloc[-1]\n",
    "        # Don't count timeouts as real trades (they're incomplete)\n",
    "    \n",
    "    # Calculate profit/loss (GROSS - before costs)\n",
    "    if signal_type == 'BUY':\n",
    "        pnl_gross = exit_price - entry_price\n",
    "        pnl_pct_gross = (pnl_gross / entry_price) * 100\n",
    "    else:  # SELL\n",
    "        pnl_gross = entry_price - exit_price\n",
    "        pnl_pct_gross = (pnl_gross / entry_price) * 100\n",
    "    \n",
    "    # ============================================================\n",
    "    # APPLY TRANSACTION COSTS\n",
    "    # ============================================================\n",
    "    # Deduct costs from gross P&L to get net P&L\n",
    "    pnl_net = pnl_gross - transaction_cost_per_trade\n",
    "    pnl_pct_net = (pnl_net / entry_price) * 100\n",
    "    \n",
    "    # Determine win/loss status AFTER costs (realistic)\n",
    "    is_win_net = pnl_net > 0\n",
    "    # ============================================================\n",
    "    \n",
    "    # Record the trade\n",
    "    trades.append({\n",
    "        'Entry_Date': entry_date,\n",
    "        'Signal_Type': signal_type,\n",
    "        'Entry_Price': entry_price,\n",
    "        'SL_Price': sl_price,\n",
    "        'TP_Price': tp_price,\n",
    "        'Exit_Price': exit_price,\n",
    "        'Exit_Reason': exit_reason,\n",
    "        'PnL_Gross': pnl_gross,                    # P&L before costs\n",
    "        'PnL_Net': pnl_net,                        # P&L after costs (REAL)\n",
    "        'Transaction_Cost': transaction_cost_per_trade,\n",
    "        'PnL_Pct_Gross': pnl_pct_gross,\n",
    "        'PnL_Pct_Net': pnl_pct_net,                # Real percentage return\n",
    "        'Bars_Held': bars_held,\n",
    "        'Is_Win_Gross': pnl_gross > 0,             # Win before costs\n",
    "        'Is_Win_Net': is_win_net,                  # Win after costs (REAL)\n",
    "        'Is_Completed': exit_reason in ['SL', 'TP']\n",
    "    })\n",
    "\n",
    "# Filter to only completed trades (those that hit SL or TP)\n",
    "completed_trades = [t for t in trades if t['Is_Completed']]\n",
    "trades_df = pd.DataFrame(completed_trades)\n",
    "\n",
    "print(f\"\\n✓ Trade Simulation Complete\")\n",
    "print(f\"  Total signals: {len(signals)}\")\n",
    "print(f\"  Total trades executed: {len(trades)}\")\n",
    "print(f\"  Completed trades (hit SL/TP): {len(trades_df)}\")\n",
    "print(f\"  Incomplete trades (timeout): {len(trades) - len(trades_df)}\")\n",
    "\n",
    "# ================================================================================\n",
    "# CALCULATE STATISTICS - BEFORE AND AFTER COSTS\n",
    "# ================================================================================\n",
    "if len(trades_df) > 0:\n",
    "    \n",
    "    # ============================================================\n",
    "    # BEFORE COSTS (Gross/Theoretical)\n",
    "    # ============================================================\n",
    "    winning_trades_gross = trades_df['Is_Win_Gross'].sum()\n",
    "    losing_trades_gross = (~trades_df['Is_Win_Gross']).sum()\n",
    "    win_rate_gross = (winning_trades_gross / len(trades_df)) * 100\n",
    "    \n",
    "    avg_win_gross = trades_df[trades_df['Is_Win_Gross']]['PnL_Gross'].mean()\n",
    "    avg_loss_gross = trades_df[~trades_df['Is_Win_Gross']]['PnL_Gross'].mean()\n",
    "    \n",
    "    total_pnl_gross = trades_df['PnL_Gross'].sum()\n",
    "    gross_profit_gross = trades_df[trades_df['Is_Win_Gross']]['PnL_Gross'].sum()\n",
    "    gross_loss_gross = abs(trades_df[~trades_df['Is_Win_Gross']]['PnL_Gross'].sum())\n",
    "    \n",
    "    profit_factor_gross = gross_profit_gross / gross_loss_gross if gross_loss_gross > 0 else 0\n",
    "    \n",
    "    # ============================================================\n",
    "    # AFTER COSTS (Net/Realistic) - THIS IS WHAT MATTERS!\n",
    "    # ============================================================\n",
    "    winning_trades_net = trades_df['Is_Win_Net'].sum()\n",
    "    losing_trades_net = (~trades_df['Is_Win_Net']).sum()\n",
    "    win_rate_net = (winning_trades_net / len(trades_df)) * 100\n",
    "    \n",
    "    avg_win_net = trades_df[trades_df['Is_Win_Net']]['PnL_Net'].mean()\n",
    "    avg_loss_net = trades_df[~trades_df['Is_Win_Net']]['PnL_Net'].mean()\n",
    "    \n",
    "    total_pnl_net = trades_df['PnL_Net'].sum()\n",
    "    gross_profit_net = trades_df[trades_df['Is_Win_Net']]['PnL_Net'].sum()\n",
    "    gross_loss_net = abs(trades_df[~trades_df['Is_Win_Net']]['PnL_Net'].sum())\n",
    "    \n",
    "    profit_factor_net = gross_profit_net / gross_loss_net if gross_loss_net > 0 else 0\n",
    "    \n",
    "    # ============================================================\n",
    "    # ADDITIONAL METRICS\n",
    "    # ============================================================\n",
    "    total_costs = trades_df['Transaction_Cost'].sum()\n",
    "    avg_bars_held = trades_df['Bars_Held'].mean()\n",
    "    \n",
    "    # Sharpe ratio (using NET returns - realistic)\n",
    "    returns_net = trades_df['PnL_Pct_Net']\n",
    "    sharpe_net = (returns_net.mean() / returns_net.std()) * np.sqrt(252) if returns_net.std() > 0 else 0\n",
    "    \n",
    "    # Calculate breakeven win rate (after costs)\n",
    "    breakeven_win_rate = abs(avg_loss_net) / (avg_win_net + abs(avg_loss_net)) * 100 if (avg_win_net + abs(avg_loss_net)) != 0 else 0\n",
    "    edge_over_breakeven = win_rate_net - breakeven_win_rate\n",
    "    \n",
    "    # ============================================================\n",
    "    # DISPLAY RESULTS\n",
    "    # ============================================================\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"BASELINE STRATEGY RESULTS (YOUR 3-CANDLE PATTERN)\")\n",
    "    print(f\"=\"*80)\n",
    "    \n",
    "    # --------------------------------------------------------\n",
    "    # SECTION 1: BEFORE TRANSACTION COSTS (Theoretical)\n",
    "    # --------------------------------------------------------\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"BEFORE TRANSACTION COSTS (Theoretical Performance)\")\n",
    "    print(f\"=\"*80)\n",
    "    print(f\"\\nTrade Count:\")\n",
    "    print(f\"  Total Completed Trades: {len(trades_df):,}\")\n",
    "    print(f\"  Winning Trades: {winning_trades_gross:,} ({win_rate_gross:.2f}%)\")\n",
    "    print(f\"  Losing Trades: {losing_trades_gross:,} ({100-win_rate_gross:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nProfitability (Gross):\")\n",
    "    print(f\"  Total P&L: ${total_pnl_gross:.2f}\")\n",
    "    print(f\"  Gross Profit: ${gross_profit_gross:.2f}\")\n",
    "    print(f\"  Gross Loss: ${gross_loss_gross:.2f}\")\n",
    "    print(f\"  Profit Factor: {profit_factor_gross:.2f}\")\n",
    "    \n",
    "    print(f\"\\nAverage Trade (Gross):\")\n",
    "    print(f\"  Avg Win: ${avg_win_gross:.4f}\")\n",
    "    print(f\"  Avg Loss: ${avg_loss_gross:.4f}\")\n",
    "    print(f\"  Win/Loss Ratio: {avg_win_gross/abs(avg_loss_gross) if avg_loss_gross != 0 else 0:.2f}\")\n",
    "    \n",
    "    # --------------------------------------------------------\n",
    "    # SECTION 2: AFTER TRANSACTION COSTS (Realistic) ← CRITICAL!\n",
    "    # --------------------------------------------------------\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"AFTER TRANSACTION COSTS (REALISTIC Performance) ← USE THIS!\")\n",
    "    print(f\"=\"*80)\n",
    "    print(f\"\\nTransaction Cost Configuration:\")\n",
    "    print(f\"  Broker Type: {SELECTED_BROKER.upper()}\")\n",
    "    print(f\"  Cost per Trade: ${transaction_cost_per_trade:.2f}\")\n",
    "    print(f\"  Total Costs Paid: ${total_costs:.2f}\")\n",
    "    \n",
    "    print(f\"\\nTrade Count:\")\n",
    "    print(f\"  Total Completed Trades: {len(trades_df):,}\")\n",
    "    print(f\"  Winning Trades: {winning_trades_net:,} ({win_rate_net:.2f}%)\")\n",
    "    print(f\"  Losing Trades: {losing_trades_net:,} ({100-win_rate_net:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nProfitability (Net):\")\n",
    "    print(f\"  Total P&L: ${total_pnl_net:.2f}\")\n",
    "    print(f\"  Gross Profit: ${gross_profit_net:.2f}\")\n",
    "    print(f\"  Gross Loss: ${gross_loss_net:.2f}\")\n",
    "    print(f\"  Profit Factor: {profit_factor_net:.2f} (>1.2 is good)\")\n",
    "    \n",
    "    print(f\"\\nAverage Trade (Net):\")\n",
    "    print(f\"  Avg Win: ${avg_win_net:.4f}\")\n",
    "    print(f\"  Avg Loss: ${avg_loss_net:.4f}\")\n",
    "    print(f\"  Win/Loss Ratio: {avg_win_net/abs(avg_loss_net) if avg_loss_net != 0 else 0:.2f}\")\n",
    "    print(f\"  Avg Bars Held: {avg_bars_held:.0f} minutes\")\n",
    "    \n",
    "    print(f\"\\nRisk-Adjusted:\")\n",
    "    print(f\"  Sharpe Ratio: {sharpe_net:.4f}\")\n",
    "    print(f\"  Return per Trade (avg): {returns_net.mean():.6f}%\")\n",
    "    \n",
    "    print(f\"\\nPer 100 Trades:\")\n",
    "    print(f\"  Net Profit: ${(total_pnl_net/len(trades_df))*100:.2f}\")\n",
    "    print(f\"  Total Costs: ${(total_costs/len(trades_df))*100:.2f}\")\n",
    "    \n",
    "    # --------------------------------------------------------\n",
    "    # SECTION 3: COST IMPACT ANALYSIS\n",
    "    # --------------------------------------------------------\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"COST IMPACT ANALYSIS\")\n",
    "    print(f\"=\"*80)\n",
    "    \n",
    "    profit_lost_to_costs = total_pnl_gross - total_pnl_net\n",
    "    profit_lost_pct = (profit_lost_to_costs / total_pnl_gross * 100) if total_pnl_gross > 0 else 0\n",
    "    \n",
    "    print(f\"\\nProfit Erosion:\")\n",
    "    print(f\"  Profit Before Costs: ${total_pnl_gross:.2f}\")\n",
    "    print(f\"  Profit After Costs: ${total_pnl_net:.2f}\")\n",
    "    print(f\"  Lost to Costs: ${profit_lost_to_costs:.2f} ({profit_lost_pct:.1f}%)\")\n",
    "    \n",
    "    win_rate_change = win_rate_gross - win_rate_net\n",
    "    trades_flipped = abs(winning_trades_gross - winning_trades_net)\n",
    "    \n",
    "    print(f\"\\nWin Rate Impact:\")\n",
    "    print(f\"  Before Costs: {win_rate_gross:.2f}%\")\n",
    "    print(f\"  After Costs: {win_rate_net:.2f}%\")\n",
    "    print(f\"  Change: {win_rate_change:+.2f}%\")\n",
    "    print(f\"  Trades Flipped to Losses: {trades_flipped} ({trades_flipped/len(trades_df)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nEdge Analysis:\")\n",
    "    print(f\"  Breakeven Win Rate (after costs): {breakeven_win_rate:.2f}%\")\n",
    "    print(f\"  Your Win Rate: {win_rate_net:.2f}%\")\n",
    "    print(f\"  Edge Over Breakeven: {edge_over_breakeven:+.2f}%\")\n",
    "    \n",
    "    if edge_over_breakeven > 0:\n",
    "        print(f\"  → POSITIVE EDGE EXISTS ✓\")\n",
    "    else:\n",
    "        print(f\"  → NO EDGE (strategy loses money after costs) ✗\")\n",
    "    \n",
    "    # --------------------------------------------------------\n",
    "    # SECTION 4: BROKER COMPARISON\n",
    "    # --------------------------------------------------------\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"BROKER COMPARISON (What-If Analysis)\")\n",
    "    print(f\"=\"*80)\n",
    "    \n",
    "    # Calculate what results would be with different broker\n",
    "    if SELECTED_BROKER == 'standard':\n",
    "        alt_broker = 'ecn'\n",
    "        alt_cost = BROKER_COSTS['ecn']\n",
    "    else:\n",
    "        alt_broker = 'standard'\n",
    "        alt_cost = BROKER_COSTS['standard']\n",
    "    \n",
    "    # Estimate performance with alternative broker\n",
    "    cost_difference = transaction_cost_per_trade - alt_cost\n",
    "    alt_total_pnl = total_pnl_net + (cost_difference * len(trades_df))\n",
    "    alt_avg_win = avg_win_net + cost_difference if winning_trades_net > 0 else 0\n",
    "    alt_avg_loss = avg_loss_net - cost_difference if losing_trades_net > 0 else 0\n",
    "    \n",
    "    # Recalculate win rate with alt broker (some losses might become wins)\n",
    "    potential_flips = int(abs(cost_difference / (avg_win_gross + abs(avg_loss_gross)) * len(trades_df)))\n",
    "    alt_winning_trades = min(winning_trades_net + potential_flips, len(trades_df))\n",
    "    alt_win_rate = (alt_winning_trades / len(trades_df)) * 100\n",
    "    \n",
    "    improvement_pct = ((alt_total_pnl - total_pnl_net) / abs(total_pnl_net) * 100) if total_pnl_net != 0 else 0\n",
    "    \n",
    "    print(f\"\\nCurrent Broker ({SELECTED_BROKER.upper()}):\")\n",
    "    print(f\"  Cost per Trade: ${transaction_cost_per_trade:.2f}\")\n",
    "    print(f\"  Win Rate: {win_rate_net:.2f}%\")\n",
    "    print(f\"  Total P&L: ${total_pnl_net:.2f}\")\n",
    "    print(f\"  Profit per 100 Trades: ${(total_pnl_net/len(trades_df))*100:.2f}\")\n",
    "    \n",
    "    print(f\"\\nAlternative Broker ({alt_broker.upper()}):\")\n",
    "    print(f\"  Cost per Trade: ${alt_cost:.2f}\")\n",
    "    print(f\"  Estimated Win Rate: ~{alt_win_rate:.2f}%\")\n",
    "    print(f\"  Estimated Total P&L: ${alt_total_pnl:.2f}\")\n",
    "    print(f\"  Estimated Profit per 100 Trades: ${(alt_total_pnl/len(trades_df))*100:.2f}\")\n",
    "    \n",
    "    print(f\"\\nSavings with {alt_broker.upper()} Broker:\")\n",
    "    print(f\"  Cost Savings per Trade: ${abs(cost_difference):.2f}\")\n",
    "    print(f\"  Total Savings: ${abs(cost_difference) * len(trades_df):.2f}\")\n",
    "    print(f\"  Profit Improvement: {improvement_pct:+.1f}%\")\n",
    "    \n",
    "    if alt_total_pnl > total_pnl_net:\n",
    "        print(f\"  → Recommendation: Switch to {alt_broker.upper()} broker ✓\")\n",
    "    else:\n",
    "        print(f\"  → Current broker is better ✓\")\n",
    "    \n",
    "    # --------------------------------------------------------\n",
    "    # SECTION 5: STRATEGY EDGE ASSESSMENT\n",
    "    # --------------------------------------------------------\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"STRATEGY EDGE ASSESSMENT (Based on Net Performance)\")\n",
    "    print(f\"=\"*80)\n",
    "    \n",
    "    # Assessment based on NET metrics (after costs)\n",
    "    edge_checks = []\n",
    "    \n",
    "    if win_rate_net >= 40:\n",
    "        print(f\"✓ Win rate {win_rate_net:.2f}% >= 40% threshold (profitable with 1:2 risk-reward)\")\n",
    "        edge_checks.append(True)\n",
    "    else:\n",
    "        print(f\"✗ Win rate {win_rate_net:.2f}% < 40% threshold (marginal edge)\")\n",
    "        edge_checks.append(False)\n",
    "    \n",
    "    if profit_factor_net >= 1.2:\n",
    "        print(f\"✓ Profit factor {profit_factor_net:.2f} >= 1.2 (good)\")\n",
    "        edge_checks.append(True)\n",
    "    else:\n",
    "        print(f\"✗ Profit factor {profit_factor_net:.2f} < 1.2 (weak)\")\n",
    "        edge_checks.append(False)\n",
    "    \n",
    "    if sharpe_net >= 0.7:\n",
    "        print(f\"✓ Sharpe ratio {sharpe_net:.4f} >= 0.7 (acceptable)\")\n",
    "        edge_checks.append(True)\n",
    "    else:\n",
    "        print(f\"✗ Sharpe ratio {sharpe_net:.4f} < 0.7 (poor risk-adjusted returns)\")\n",
    "        edge_checks.append(False)\n",
    "    \n",
    "    if edge_over_breakeven > 2.0:\n",
    "        print(f\"✓ Edge over breakeven {edge_over_breakeven:.2f}% > 2% (good cushion)\")\n",
    "        edge_checks.append(True)\n",
    "    else:\n",
    "        print(f\"⚠ Edge over breakeven {edge_over_breakeven:.2f}% < 2% (tight margin)\")\n",
    "        edge_checks.append(False)\n",
    "    \n",
    "    # Overall assessment\n",
    "    edge_score = sum(edge_checks)\n",
    "    \n",
    "    print(f\"\\nEdge Score: {edge_score}/4\")\n",
    "    \n",
    "    if edge_score >= 3:\n",
    "        print(f\"→ STRONG EDGE: Strategy shows statistically significant advantage ✓\")\n",
    "        print(f\"→ ML MODEL should aim to beat this baseline\")\n",
    "        print(f\"→ Strategy is READY for further development\")\n",
    "    elif edge_score >= 2:\n",
    "        print(f\"→ MODERATE EDGE: Strategy is profitable but needs optimization ⚠\")\n",
    "        print(f\"→ Consider: ECN broker, volatility filters, or better entry timing\")\n",
    "        print(f\"→ ML MODEL has room for improvement\")\n",
    "    else:\n",
    "        print(f\"→ WEAK/NO EDGE: Strategy is marginal after costs ✗\")\n",
    "        print(f\"→ Recommendation: Major changes needed before proceeding\")\n",
    "        print(f\"→ Consider: Different pattern, timeframe, or instrument\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"⚠ CRITICAL: Always use NET (after-cost) performance for decisions!\")\n",
    "    print(f\"⚠ Gross performance is misleading and will cause losses in live trading!\")\n",
    "    print(f\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb649e1",
   "metadata": {},
   "source": [
    "### Transaction Costs: Included\n",
    "```\n",
    "BROKER_COSTS = {\n",
    "    'standard': 0.45,  # ← This IS transaction cost\n",
    "    'ecn': 0.20        # ← This IS transaction cost\n",
    "}\n",
    "```\n",
    "\n",
    "### Spread: Included (part of transaction cost)\n",
    "```\n",
    "'standard': 0.45  # = $0.30 spread + $0.15 slippage\n",
    "'ecn': 0.20       # = $0.05 spread + $0.15 slippage\n",
    "```\n",
    "\n",
    "\n",
    "### Slippage: Include (part of transaction cost)\n",
    "- The $0.45 or $0.20 already contains slippage estimate\n",
    "- From Section 14:\n",
    "- Standard broker: $0.30 spread + $0.15 slippage = $0.45\n",
    "- ECN broker: $0.05 spread + $0.15 slippage = $0.20\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Breakdown of What Each Cost Represents:\n",
    "\n",
    "### **Standard Broker ($0.45 total):**\n",
    "```\n",
    "Entry:\n",
    "- Spread cost: $0.30 (you pay 3 pips to enter)\n",
    "\n",
    "Exit:\n",
    "- Spread cost: $0.00 (already included in entry)\n",
    "- Slippage: $0.15 (execution delay, price moves)\n",
    "\n",
    "Total: $0.45 per round-trip trade\n",
    "```\n",
    "\n",
    "### **ECN Broker ($0.20 total):**\n",
    "```\n",
    "Entry:\n",
    "- Spread cost: $0.05 (you pay 0.5 pips to enter)\n",
    "- Commission: $0.035 (fixed fee per 0.01 lot)\n",
    "\n",
    "Exit:\n",
    "- Spread cost: $0.00 (minimal)\n",
    "- Commission: $0.035 (fixed fee per 0.01 lot)\n",
    "- Slippage: $0.10 (better execution, less slippage)\n",
    "\n",
    "Total: $0.20 per round-trip trade\n",
    "\n",
    "\n",
    "### What this means:\n",
    "- When you see this in output:\n",
    "`pnl_net = pnl_gross - transaction_cost_per_trade`\n",
    "\n",
    "- It means:\n",
    "`pnl_net = raw_profit - (spread + slippage + commission)`\n",
    "\n",
    "**Example:**\n",
    "\n",
    "- Raw profit: $2.00\n",
    "- Costs: $0.45\n",
    "- Net profit: $2.00 - $0.45 = $1.55 ← THIS is what you ACTUALLY make"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".allenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
